{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, TransfoXLTokenizer, TFTransfoXLLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(model_info, model_name, context, joint_vocab):\n",
    "\n",
    "        model, tokenizer = model_info[model_name]\n",
    "\n",
    "        input = tokenizer(context,return_tensors='tf')\n",
    "        outputs = model(input)\n",
    "\n",
    "        probabilities = softmax(outputs[0])\n",
    "        ids = range(0,len(probabilities[0][0]))\n",
    "        vocab = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "        distr_dict = dict(zip(vocab, probabilities[0][0]))\n",
    "\n",
    "        return distr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def js(p, q):\n",
    "\n",
    "    intersection = p.keys() & q.keys()\n",
    "    p = {k:v for k,v in p.items() if k in intersection}\n",
    "    q = {k:v for k,v in q.items() if k in intersection}\n",
    "\n",
    "    p = [v for k, v in sorted(p.items())]\n",
    "    q = [v for k, v in sorted(q.items())]\n",
    "\n",
    "    p = np.asarray(p)\n",
    "    q = np.asarray(q)\n",
    "    # normalize\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "    m = (p + q) / 2\n",
    "    return (entropy(p, m) + entropy(q, m)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_regressive(model_info, curr_context, num_return_seqs, current_len, max_len, total_js, joint_vocab):\n",
    "\n",
    "    if current_len == max_len:\n",
    "        return total_js / current_len\n",
    "\n",
    "    distrs = {}\n",
    "    highest = {}\n",
    "    for model_name in ['GPT2','TransformerXL']:\n",
    "        model, tokenizer = model_info[model_name]\n",
    "        next_word_distr = get_distribution(model_info, model_name, curr_context, joint_vocab)\n",
    "        distrs[model_name] = next_word_distr\n",
    "\n",
    "    A = distrs['GPT2']\n",
    "    B = distrs['TransformerXL']\n",
    "    # average the two distributions\n",
    "    avg_distr = {x: (A.get(x, 0) + B.get(x, 0))/2 for x in set(A).intersection(B)}\n",
    "    \n",
    "    js_dict = {}\n",
    "    for i in range(0,5):\n",
    "        n = random.randint(0,len(avg_distr))\n",
    "        new_word = avg_distr.items()[n][0]\n",
    "        print(\"NEW WORD\", new_word)\n",
    "        print(\"CURR PRE-NEW CONTEXT\", curr_context)\n",
    "        new_context =  curr_context + new_word\n",
    "        print(\"NEW CONTEXT\", new_context)\n",
    "        p = get_distribution(model_info, 'GPT2', new_context, joint_vocab)\n",
    "        q = get_distribution(model_info,'TransformerXL', new_context, joint_vocab)\n",
    "        print(p,q)\n",
    "        js_result = js(p,q)\n",
    "        js_dict[new_word] = js_result\n",
    "\n",
    "\n",
    "    highest_js_word = sorted(js_dict.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "    print(\"highest JS word\", highest_js_word)\n",
    "    curr_context = curr_context + highest_js_word\n",
    "\n",
    "    p = get_distribution(model_info,'GPT2', curr_context, joint_vocab)\n",
    "    q = get_distribution(model_info,'TransformerXL', curr_context, joint_vocab)\n",
    "\n",
    "    total_js += js(p,q)\n",
    "    print(\"CURR CONTEXT\", curr_context, \"JS\", total_js)\n",
    "    # then autoregressive again on this new current context\n",
    "    return auto_regressive(model_info, curr_context, num_return_seqs, current_len + 1, max_len , total_js)\n",
    "\n",
    "model_info = {\"GPT2\": (TFGPT2LMHeadModel.from_pretrained(\"gpt2\"),GPT2Tokenizer.from_pretrained(\"gpt2\")), \n",
    "              \"TransformerXL\": (TFTransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103'),TransfoXLTokenizer.from_pretrained('transfo-xl-wt103'))}\n",
    "\n",
    "curr_context = \"What\"\n",
    "gpt2_dict = get_distribution(model_info, \"GPT2\", curr_ontext, {})\n",
    "txl_dict = get_distribution(model_info, \"TransformerXL\" curr_context, {})\n",
    "\n",
    "joint_vocab = gpt2_dict.keys() & txl_dict.keys()\n",
    "\n",
    "auto_regressive(model_info, curr_context, 50, 1, 15, 0, joint_vocab)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\"GPT2\": (TFGPT2LMHeadModel.from_pretrained(\"gpt2\"),GPT2Tokenizer.from_pretrained(\"gpt2\")), \"TransformerXL\": (TFTransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103'),TransfoXLTokenizer.from_pretrained('transfo-xl-wt103'))}\n",
    "curr_context = \"What\"\n",
    "\n",
    "auto_regressive(model_info, curr_context, 50, 1, 15, 0, )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
