ModuleCmd_Load.c(213):ERROR:105: Unable to locate a modulefile for 'python'
2020-09-07 13:03:21.326491: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:03:21.326574: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:03:21.326582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-09-07 13:03:25.799552: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-09-07 13:03:25.842458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-07 13:03:25.846136: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-09-07 13:03:25.846180: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: node082
2020-09-07 13:03:25.846188: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: node082
2020-09-07 13:03:25.846272: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.82.0
2020-09-07 13:03:25.846309: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.82.0
2020-09-07 13:03:25.846317: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.82.0
2020-09-07 13:03:25.846674: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-09-07 13:03:25.856130: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-09-07 13:03:25.857024: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56284bb7bc30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-07 13:03:25.857049: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
All model checkpoint weights were used when initializing TFGPT2LMHeadModel.

All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
All model checkpoint weights were used when initializing TFTransfoXLLMHeadModel.

All the weights of TFTransfoXLLMHeadModel were initialized from the model checkpoint at transfo-xl-wt103.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFTransfoXLLMHeadModel for predictions without further training.
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
NEW WORD  he
CURR PRE-NEW CONTEXT Who
NEW CONTEXT Who he
{' was': 0.1, ' can': 0.04, ' likes': 0.02, ' had': 0.06, "'s": 0.14, ' does': 0.04, ' says': 0.06, ' is': 0.12, ' said': 0.1, ' did': 0.04, ' will': 0.02, ' should': 0.02, ' knows': 0.02, ' loves': 0.06, ' looks': 0.04, ' calls': 0.02, ' has': 0.02, ' told': 0.02, ' thought': 0.02, ' thinks': 0.02, ' knew': 0.02} {' is': 0.36, ' was': 0.5, ' became': 0.02, ' belongs': 0.02, ' would': 0.04, ' has': 0.02, ' had': 0.02, '?': 0.02}
NEW WORD  you
CURR PRE-NEW CONTEXT Who
NEW CONTEXT Who you
{' do': 0.02, "'re": 0.12, ' can': 0.06, ' want': 0.02, ' should': 0.08, ' are': 0.1, ' see': 0.08, ' need': 0.06, ' say': 0.04, ' get': 0.04, ' really': 0.02, ' may': 0.02, ' guys': 0.02, ' got': 0.02, ' might': 0.04, ' will': 0.06, "'ve": 0.04, ' know': 0.02, ' have': 0.06, "'ll": 0.02, ' think': 0.02, ' find': 0.02, ' would': 0.02} {"'s": 0.02, ' the': 0.04, ' who': 0.04, ' "': 0.04, '?': 0.22, ' and': 0.1, ' were': 0.1, ',': 0.08, ' most': 0.02, ' I': 0.04, ' with': 0.04, ' are': 0.04, ' would': 0.04, ' you': 0.02, ' that': 0.02, ' meet': 0.02, ' shouldn': 0.02, ' in': 0.02, ' should': 0.02, '.': 0.02, ' really': 0.02, ' have': 0.02}
NEW WORD  do
CURR PRE-NEW CONTEXT Who
NEW CONTEXT Who do
{' you': 0.92, ' we': 0.06, ' they': 0.02} {' who': 0.22, '?': 0.42, ',': 0.08, ' was': 0.04, ' Guys': 0.02, ' whom': 0.04, ' the': 0.08, ' we': 0.02, ' they': 0.02, ' their': 0.02, ' I': 0.02, ' "': 0.02}
NEW WORD  (
CURR PRE-NEW CONTEXT Who
NEW CONTEXT Who (
{'Is': 0.08, 'A': 0.06, 'What': 0.06, 'or': 0.04, 'Who': 0.04, 'Why': 0.02, 'are': 0.08, 's': 0.06, 'and': 0.06, 'who': 0.1, 'could': 0.02, 'Are': 0.06, 'if': 0.04, 'You': 0.02, 'which': 0.02, 'what': 0.04, 'can': 0.02, 'i': 0.02, 'has': 0.04, 'not': 0.02, 'will': 0.02, 'Do': 0.02, 'I': 0.04, 'is': 0.02} {' )': 0.46, '?': 0.08, ',': 0.1, ' who': 0.08, ' the': 0.08, ' and': 0.06, ' whom': 0.04, ' of': 0.04, ' or': 0.04, ' s': 0.02}
NEW WORD  the
CURR PRE-NEW CONTEXT Who
NEW CONTEXT Who the
{' heck': 0.24, ' Hell': 0.06, ' hell': 0.56, ' Heck': 0.04, ' fuck': 0.1} {'': 0.02, ' Angels': 0.12, ' was': 0.06, ' boys': 0.02, ' Guys': 0.08, ' Song': 0.04, ' group': 0.02, ' Was': 0.04, ' Men': 0.04, ' is': 0.04, ' Nation': 0.04, ' "': 0.06, ' song': 0.02, ' and': 0.04, ' Who': 0.04, "'s": 0.02, ' dictates': 0.02, ' The': 0.04, ' performers': 0.02, ' Fountain': 0.02, '?': 0.06, ' the': 0.04, ' where': 0.04, ' Band': 0.02, ' team': 0.02, ' I': 0.02}
{' he': 0.060747199016978344, ' you': 0.058206032104001035, ' do': 0.033822075568605205, ' (': 0.0, ' the': 0.0}
JS List [(' (', 0.0), (' the', 0.0), (' do', 0.033822075568605205), (' you', 0.058206032104001035), (' he', 0.060747199016978344)]
highest JS word  he
CURR CONTEXT Who he JS 0.0396426237574815
NEW WORD  says
CURR PRE-NEW CONTEXT Who he
NEW CONTEXT Who he says
{' "': 0.06, ' is': 0.12, ' on': 0.02, '?"': 0.04, ':': 0.08, ' he': 0.14, ' was': 0.06, ' the': 0.04, ' about': 0.06, ' or': 0.02, ',': 0.18, ' that': 0.08, ' I': 0.02, ' to': 0.02, ' they': 0.02, '...': 0.02, ' has': 0.02} {' is': 0.6, ' has': 0.04, ' was': 0.2, ' calls': 0.04, ',': 0.02, ' plays': 0.02, ' Was': 0.02, '?': 0.04, ' to': 0.02}
NEW WORD  really
CURR PRE-NEW CONTEXT Who he
NEW CONTEXT Who he really
{' is': 0.48, ' was': 0.16, ' wants': 0.08, ' did': 0.02, ' does': 0.04, ' loves': 0.02, ' means': 0.02, ' wanted': 0.08, ' meant': 0.06, ' believed': 0.04} {' is': 0.56, ' was': 0.4, ' really': 0.02, ' and': 0.02}
NEW WORD  should
CURR PRE-NEW CONTEXT Who he
NEW CONTEXT Who he should
{' be': 0.36, ' have': 0.16, ' call': 0.02, ' do': 0.1, ' read': 0.02, "'ve": 0.06, ' think': 0.02, ' pay': 0.08, ' go': 0.02, ' talk': 0.02, ' play': 0.02, ' really': 0.02, ' vote': 0.02, ' know': 0.02, ' keep': 0.02, ' use': 0.02, ' try': 0.02} {' have': 0.5, ' be': 0.38, ' are': 0.02, '?': 0.04, ',': 0.02, ' am': 0.02, ' and': 0.02}
NEW WORD  meets
CURR PRE-NEW CONTEXT Who he
NEW CONTEXT Who he meets
{' is': 0.18, ' later': 0.02, ';': 0.02, ' may': 0.02, ',': 0.24, ' and': 0.02, ' will': 0.1, '.': 0.06, '?"': 0.02, ' here': 0.04, ' there': 0.02, ' can': 0.04, ' in': 0.04, ' now': 0.02, ' with': 0.02, ':': 0.02, ' at': 0.04, '?': 0.02, ' on': 0.02, ' during': 0.02, ' (': 0.02} {' is': 0.08, ' with': 0.36, '.': 0.22, ' and': 0.1, ' after': 0.04, ' was': 0.04, '?': 0.04, ' has': 0.02, ' about': 0.02, ' â\x80\x93': 0.02, ' when': 0.02, ' in': 0.02, ' while': 0.02}
NEW WORD  wants
CURR PRE-NEW CONTEXT Who he
NEW CONTEXT Who he wants
{' to': 0.52, ' and': 0.02, ' is': 0.14, ' in': 0.02, ' on': 0.02, ',"': 0.04, ' the': 0.04, '.': 0.04, ' his': 0.02, ',': 0.04, ' isn': 0.02, ' at': 0.02, ':': 0.02, ' now': 0.04} {' to': 0.68, '.': 0.02, ' is': 0.04, ' was': 0.08, ',': 0.08, ' â\x80\x93': 0.04, ' for': 0.02, ' as': 0.02, ' and': 0.02}
{' says': 0.16303865179068436, ' really': 0.015749962989645705, ' should': 0.03489246015737321, ' meets': 0.19118605197998642, ' wants': 0.03004475396189832}
JS List [(' really', 0.015749962989645705), (' wants', 0.03004475396189832), (' should', 0.03489246015737321), (' says', 0.16303865179068436), (' meets', 0.19118605197998642)]
highest JS word  meets
CURR CONTEXT Who he meets JS 0.04116008390301991
NEW WORD  â
CURR PRE-NEW CONTEXT Who he meets
NEW CONTEXT Who he meets â
{'�': 0.26, 'n': 0.06, 's': 0.04, ' is': 0.06, "'": 0.04, ' (': 0.04, ',': 0.12, ' and': 0.02, 't': 0.04, '´': 0.06, 'í': 0.02, ' �': 0.06, 'á': 0.02, '"': 0.06, ' the': 0.02, ' has': 0.02, '?': 0.02, ' ': 0.02, 'é': 0.02} {' for': 0.02, ',': 0.14, '.': 0.2, ' or': 0.02, ' when': 0.02, '?': 0.08, ' with': 0.04, ' who': 0.08, ' was': 0.08, ' â\x80\x94': 0.02, ' the': 0.06, ' is': 0.1, ' a': 0.02, ' had': 0.02, ' and': 0.04, ' has': 0.04, ' became': 0.02}
NEW WORD :
CURR PRE-NEW CONTEXT Who he meets
NEW CONTEXT Who he meets:
{' A': 0.08, ' "': 0.08, ' The': 0.14, '\n': 0.16, ' John': 0.06, ' His': 0.02, ' An': 0.04, ' It': 0.02, ' Bill': 0.02, ' Jack': 0.06, ' H': 0.02, ' In': 0.02, ' Dr': 0.02, ' the': 0.06, ' Joseph': 0.02, ' M': 0.02, ' Bob': 0.02, ' I': 0.02, ' He': 0.08, ' Tom': 0.02, ' a': 0.02} {'Who he the': 0.04, 'Who he with': 0.16, 'Who he was': 0.26, 'Who he â\x80\x93': 0.02, 'Who he Was': 0.02, 'Who he â\x80\x94': 0.04, 'Who he when': 0.02, 'Who he and': 0.08, 'Who he,': 0.1, 'Who he to': 0.06, 'Who he would': 0.04, 'Who he?': 0.02, 'Who he': 0.02, 'Who he.': 0.08, 'Who he in': 0.02, 'Who he (': 0.02}
NEW WORD  now
CURR PRE-NEW CONTEXT Who he meets
NEW CONTEXT Who he meets now
{',': 0.22, ' has': 0.06, '?"': 0.04, '.': 0.02, ' will': 0.12, ' means': 0.04, ' is': 0.26, ' does': 0.02, ' in': 0.02, ' at': 0.02, ':': 0.02, ',"': 0.02, ' and': 0.04, ' on': 0.02, ' might': 0.02, ' seems': 0.02, ' would': 0.02, '...': 0.02} {' with': 0.16, '.': 0.22, '?': 0.12, ',': 0.18, ' was': 0.06, ' and': 0.02, ' he': 0.02, ' is': 0.16, ' his': 0.04, ' who': 0.02}
NEW WORD  at
CURR PRE-NEW CONTEXT Who he meets
NEW CONTEXT Who he meets at
{' the': 0.52, ' home': 0.02, ' this': 0.06, ' his': 0.12, ' The': 0.02, ' dinner': 0.02, ' Hogwarts': 0.02, ' school': 0.02, ' a': 0.06, ' New': 0.02, ' their': 0.02, ' these': 0.02, ' every': 0.02, ' one': 0.02, ' your': 0.02, ' least': 0.02} {' the': 0.62, ' a': 0.12, ' his': 0.1, ' meeting': 0.04, ' where': 0.02, ',': 0.02, ' end': 0.02, '.': 0.02, ' Club': 0.02, ' when': 0.02}
NEW WORD ?"
CURR PRE-NEW CONTEXT Who he meets
NEW CONTEXT Who he meets?"
{' (': 0.02, ' He': 0.02, ' he': 0.12, '\n': 0.4, ' she': 0.06, ' says': 0.04, ' asked': 0.04, ' "': 0.08, '\n\n': 0.02, ' said': 0.06, ' or': 0.02, ' The': 0.02, ' I': 0.04, ' was': 0.02, '': 0.02, ' they': 0.02} {'Who he the': 0.04, 'Who he to': 0.04, 'Who he and': 0.06, 'Who he was': 0.2, 'Who he is': 0.06, 'Who he.': 0.14, 'Who he after': 0.02, 'Who he with': 0.16, 'Who he who': 0.02, 'Who he,': 0.06, 'Who he ;': 0.02, 'Who he in': 0.02, 'Who he?': 0.06, 'Who he as': 0.02, 'Who he â\x80\x93': 0.02, 'Who he "': 0.02, 'Who he (': 0.02, 'Who he his': 0.02}
{' â\x80\x94': 0.021646526384093846, ':': 0.0, ' now': 0.10293744694722996, ' at': 0.005987825382895414, '?"': 0.0}
JS List [(':', 0.0), ('?"', 0.0), (' at', 0.005987825382895414), (' â\x80\x94', 0.021646526384093846), (' now', 0.10293744694722996)]
highest JS word  now
CURR CONTEXT Who he meets now JS 0.10946191846511895
NEW WORD :
CURR PRE-NEW CONTEXT Who he meets now
NEW CONTEXT Who he meets now:
{' President': 0.02, ' Is': 0.04, ' the': 0.04, '\n': 0.16, ' Mr': 0.06, '\n\n': 0.02, ' Tom': 0.02, ' His': 0.04, ' It': 0.02, ' The': 0.08, ' In': 0.04, ' his': 0.02, ' K': 0.04, ' Harry': 0.02, ' David': 0.06, ' Bill': 0.02, ' he': 0.02, ' He': 0.04, " '": 0.02, ' This': 0.04, ' "': 0.04, ' What': 0.04, ' A': 0.02, ' a': 0.04, ' Trump': 0.02, ' How': 0.02} {'Who he meets.': 0.16, 'Who he meets was': 0.04, 'Who he meets Is': 0.02, 'Who he meets the': 0.08, 'Who he meets?': 0.1, 'Who he meets and': 0.08, 'Who he meets is': 0.14, 'Who he meets he': 0.02, 'Who he meets with': 0.12, 'Who he meets who': 0.04, 'Who he meets ;': 0.04, 'Who he meets': 0.02, 'Who he meets :': 0.02, 'Who he meets â\x80\x93': 0.04, 'Who he meets to': 0.04, 'Who he meets,': 0.04}
NEW WORD  he
CURR PRE-NEW CONTEXT Who he meets now
NEW CONTEXT Who he meets now he
{' is': 0.18, "'s": 0.22, ' can': 0.06, ' will': 0.1, ' has': 0.06, ' knows': 0.04, ' should': 0.04, ' really': 0.02, ' gets': 0.04, ' feels': 0.02, "'ll": 0.04, ' finds': 0.02, ' seems': 0.02, ' may': 0.02, ' must': 0.02, ' doesn': 0.02, ' does': 0.02, ' tells': 0.02, ' says': 0.02, ' never': 0.02} {' is': 0.28, ' dresses': 0.02, ' feels': 0.02, ' encounters': 0.04, ' and': 0.06, ' has': 0.06, ' finds': 0.1, ' becomes': 0.04, ' would': 0.02, ' had': 0.02, ' met': 0.04, ',': 0.04, ' marries': 0.04, ' does': 0.04, ' sleeps': 0.02, ' considers': 0.02, ' learns': 0.06, ' needs': 0.04, ' calls': 0.02, ' was': 0.02}
NEW WORD ?"
CURR PRE-NEW CONTEXT Who he meets now
NEW CONTEXT Who he meets now?"
{' "': 0.16, '\n': 0.42, ' says': 0.04, ' he': 0.1, ' [': 0.02, ' the': 0.02, ' She': 0.04, ' said': 0.06, ' and': 0.02, ' He': 0.02, ' I': 0.02, ' a': 0.02, ' And': 0.02, ' The': 0.02, ' asked': 0.02} {'Who he meets is': 0.12, 'Who he meets was': 0.1, 'Who he meets?': 0.12, 'Who he meets,': 0.14, 'Who he meets with': 0.08, 'Who he meets.': 0.22, 'Who he meets who': 0.06, 'Who he meets Is': 0.02, 'Who he meets to': 0.04, 'Who he meets where': 0.04, 'Who he meets ;': 0.02, 'Who he meets (': 0.02, 'Who he meets â\x80\x93': 0.02}
NEW WORD  on
CURR PRE-NEW CONTEXT Who he meets now
NEW CONTEXT Who he meets now on
{' stage': 0.1, ' a': 0.14, ' the': 0.5, ' Sunday': 0.02, ' this': 0.02, ' Thursday': 0.04, ' camera': 0.02, ' The': 0.02, ' Monday': 0.02, ' Twitter': 0.04, ' his': 0.04, ' our': 0.02, ' Saturday': 0.02} {' Saturday': 0.18, ' on': 0.08, ' the': 0.34, ' and': 0.02, ' a': 0.04, ' Monday': 0.04, ' August': 0.02, ' Friday': 0.02, ' his': 0.08, ' Tuesday': 0.02, ' Independence': 0.02, ' with': 0.02, ' Wednesday': 0.02, ' July': 0.02, ' May': 0.02, ' October': 0.02, ' Sundays': 0.02, ' site': 0.02}
NEW WORD  isn
CURR PRE-NEW CONTEXT Who he meets now
NEW CONTEXT Who he meets now isn
{"'t": 1.0} {" 't": 1.0}
{':': 0.0, ' he': 0.02482785754622684, '?"': 0.0, ' on': 0.09108906684415495, ' isn': 0.0}
JS List [(':', 0.0), ('?"', 0.0), (' isn', 0.0), (' he', 0.02482785754622684), (' on', 0.09108906684415495)]
highest JS word  on
CURR CONTEXT Who he meets now on JS 0.12993523774374158
2020-09-07 13:17:43.666193: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:17:43.666292: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:17:43.666305: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-09-07 13:17:49.701062: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-09-07 13:17:49.745560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-07 13:17:49.749555: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-09-07 13:17:49.749609: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: node082
2020-09-07 13:17:49.749617: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: node082
2020-09-07 13:17:49.749737: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.82.0
2020-09-07 13:17:49.749775: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.82.0
2020-09-07 13:17:49.749781: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.82.0
2020-09-07 13:17:49.750164: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-09-07 13:17:49.759129: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-09-07 13:17:49.760005: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f25d794630 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-07 13:17:49.760042: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
All model checkpoint weights were used when initializing TFGPT2LMHeadModel.

All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
All model checkpoint weights were used when initializing TFTransfoXLLMHeadModel.

All the weights of TFTransfoXLLMHeadModel were initialized from the model checkpoint at transfo-xl-wt103.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFTransfoXLLMHeadModel for predictions without further training.
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
NEW WORD  they
CURR PRE-NEW CONTEXT Where
NEW CONTEXT Where they
{' came': 0.06, ' were': 0.14, ' are': 0.14, ' call': 0.06, ' did': 0.04, ' got': 0.06, ' had': 0.04, ' stand': 0.02, ' want': 0.02, ' would': 0.02, ' used': 0.02, ' get': 0.04, "'re": 0.04, ' come': 0.04, ' do': 0.06, ' went': 0.02, ' see': 0.04, ' look': 0.02, ' make': 0.02, ' put': 0.02, ' go': 0.02, ' play': 0.02, ' say': 0.02, ' can': 0.02} {' the': 0.02, ' they': 0.04, ',': 0.08, ' then': 0.02, ' had': 0.14, ' are': 0.2, ' places': 0.04, ' has': 0.02, ' were': 0.12, ' once': 0.02, ' stands': 0.02, '.': 0.02, ' lie': 0.02, ' came': 0.04, ' lay': 0.02, ' was': 0.04, ' (': 0.02, ' now': 0.04, ' have': 0.02, ' where': 0.02, ' would': 0.02, ' can': 0.02}
NEW WORD  there
CURR PRE-NEW CONTEXT Where
NEW CONTEXT Where there
{' was': 0.06, ' is': 0.36, "'s": 0.24, ' are': 0.28, ' were': 0.04, ' has': 0.02} {' is': 0.48, ' are': 0.04, ' where': 0.04, ' was': 0.22, ' had': 0.02, ' should': 0.02, ' exists': 0.04, ' stands': 0.02, ',': 0.02, ' and': 0.04, ' lies': 0.02, ' has': 0.02, ' can': 0.02}
NEW WORD  you
CURR PRE-NEW CONTEXT Where
NEW CONTEXT Where you
{' are': 0.1, ' can': 0.16, ' ask': 0.02, ' live': 0.02, ' get': 0.06, ' play': 0.04, "'re": 0.12, ' buy': 0.08, ' need': 0.02, ' look': 0.04, ' start': 0.02, ' will': 0.06, ' use': 0.02, ' come': 0.02, ' take': 0.02, ' enter': 0.02, ' feel': 0.02, ' do': 0.02, ' go': 0.04, ' see': 0.06, ' think': 0.02, ' want': 0.02} {',': 0.08, ' as': 0.02, ' place': 0.02, ' is': 0.22, ' with': 0.06, ' where': 0.04, ' yours': 0.06, ' was': 0.02, ' in': 0.02, ' and': 0.04, ' can': 0.1, ' has': 0.06, '.': 0.02, ' lies': 0.02, ' when': 0.06, ' now': 0.02, ' have': 0.04, ' would': 0.02, ' were': 0.04, ' stands': 0.02, ' just': 0.02}
NEW WORD  with
CURR PRE-NEW CONTEXT Where
NEW CONTEXT Where with
{' the': 0.38, ' a': 0.04, ' her': 0.04, ' my': 0.06, ' their': 0.06, ' that': 0.04, ' all': 0.16, ' any': 0.02, ' no': 0.02, ' every': 0.02, ' it': 0.02, ' what': 0.02, ' so': 0.02, ' this': 0.04, ' your': 0.02, ' an': 0.02, ' many': 0.02} {' the': 0.48, ' room': 0.04, ' shows': 0.04, ' space': 0.02, ' had': 0.02, ' most': 0.02, ' was': 0.02, ',': 0.04, ' has': 0.06, ' other': 0.06, ' number': 0.02, ' is': 0.02, ' where': 0.04, ' an': 0.02, ' a': 0.04, ' only': 0.02, ' one': 0.04}
NEW WORD  a
CURR PRE-NEW CONTEXT Where
NEW CONTEXT Where a
{' number': 0.04, ' person': 0.14, ' couple': 0.02, ' lot': 0.06, ' big': 0.02, ' kid': 0.02, ' few': 0.02, ' child': 0.06, ' book': 0.04, ' player': 0.08, ' single': 0.04, ' character': 0.02, ' student': 0.02, ' man': 0.08, ' new': 0.06, ' video': 0.02, ' family': 0.02, ' woman': 0.02, ' team': 0.02, ' city': 0.02, ' company': 0.02, ' guy': 0.04, ' human': 0.02, ' game': 0.02, ' "': 0.04, ' great': 0.02, ' school': 0.02} {' near': 0.04, ' place': 0.04, ' number': 0.02, ' would': 0.04, ' is': 0.12, ' two': 0.06, ' small': 0.02, ' where': 0.04, ' previous': 0.08, ' new': 0.04, ' location': 0.04, ',': 0.06, ' left': 0.06, ' nearby': 0.02, ' was': 0.08, ' (': 0.02, ' "': 0.06, '': 0.02, ' now': 0.02, ' in': 0.04, ' a': 0.02, ' had': 0.02, ' one': 0.02, ' and': 0.02}
{' they': 0.032006611766349176, ' there': 0.10637225111455412, ' you': 0.0, ' with': 0.0007362901450172835, ' a': 0.02570619323956676}
JS List [(' you', 0.0), (' with', 0.0007362901450172835), (' a', 0.02570619323956676), (' they', 0.032006611766349176), (' there', 0.10637225111455412)]
highest JS word  there
CURR CONTEXT Where there JS 0.0021791185481286005
NEW WORD  places
CURR PRE-NEW CONTEXT Where there
NEW CONTEXT Where there places
{' to': 0.12, ' where': 0.22, ' for': 0.02, ' and': 0.02, ' you': 0.02, ' I': 0.04, ' like': 0.04, ',': 0.06, ' the': 0.06, ' are': 0.16, ' a': 0.02, ' there': 0.04, ' that': 0.06, ' is': 0.02, ' in': 0.06, ' we': 0.02, ' of': 0.02} {' stands': 0.06, ' places': 0.46, ' and': 0.04, ' the': 0.06, '.': 0.02, ' to': 0.04, ',': 0.02, ' is': 0.08, ' a': 0.04, ' "': 0.02, ' where': 0.02, ' resources': 0.02, ' had': 0.02, ' an': 0.02, ' there': 0.02, ' are': 0.02, ' with': 0.02, ' place': 0.02}
NEW WORD  are
CURR PRE-NEW CONTEXT Where there
NEW CONTEXT Where there are
{' only': 0.04, ' no': 0.24, ' different': 0.04, ' too': 0.04, ' such': 0.02, ' many': 0.12, ' still': 0.06, ' people': 0.06, ' a': 0.04, ' two': 0.06, ' few': 0.02, ' more': 0.06, ' so': 0.04, ' enough': 0.02, ' any': 0.04, ' other': 0.04, ' 2': 0.02, ' five': 0.02, ' less': 0.02} {' in': 0.06, ' is': 0.12, ' many': 0.02, ' there': 0.08, ' stands': 0.02, ' not': 0.06, ' large': 0.02, ' where': 0.08, ' locations': 0.04, ' small': 0.02, ' now': 0.02, ' only': 0.04, ' two': 0.06, ' the': 0.08, ' places': 0.08, ' stations': 0.06, '.': 0.02, ' also': 0.02, ' areas': 0.04, ' no': 0.02, ' resources': 0.02, ',': 0.02}
NEW WORD  was
CURR PRE-NEW CONTEXT Where there
NEW CONTEXT Where there was
{' no': 0.18, ' a': 0.42, ' any': 0.04, ' more': 0.02, ' an': 0.06, ' never': 0.02, ' nothing': 0.06, ' such': 0.06, ' the': 0.02, ' still': 0.02, ' one': 0.02, ' some': 0.02, ' this': 0.02, ' little': 0.02, ' much': 0.02} {' is': 0.12, ' with': 0.04, ',': 0.06, ' space': 0.04, ' anywhere': 0.02, ' only': 0.04, ' has': 0.06, ' room': 0.02, ' was': 0.14, '.': 0.02, ' the': 0.06, ' not': 0.02, ' at': 0.02, ' a': 0.12, ' where': 0.06, ' an': 0.02, ' no': 0.02, ' and': 0.02, ' there': 0.04, ' so': 0.02, ' one': 0.02, ' water': 0.02}
NEW WORD  there
CURR PRE-NEW CONTEXT Where there
NEW CONTEXT Where there there
{' are': 0.32, ' is': 0.48, ' was': 0.12, "'s": 0.02, ' has': 0.04, ' were': 0.02} {' is': 0.48, ' was': 0.24, ' the': 0.02, ' and': 0.02, ' has': 0.04, ' stands': 0.06, ' a': 0.02, ' would': 0.02, ' lies': 0.02, ',': 0.04, '.': 0.02, ' places': 0.02}
NEW WORD  can
CURR PRE-NEW CONTEXT Where there
NEW CONTEXT Where there can
{' be': 0.98, "'t": 0.02} {' be': 0.36, ' was': 0.06, ' the': 0.02, ' also': 0.04, ' is': 0.16, ' now': 0.06, '.': 0.02, ' only': 0.08, ' have': 0.02, ' a': 0.02, ' with': 0.02, ' exist': 0.02, ' always': 0.02, ' where': 0.02, ',': 0.02, ' can': 0.02, ' been': 0.02, ' exists': 0.02}
{' places': 0.15950343635816683, ' are': 0.13595678916958887, ' was': 0.08302978521071844, ' there': 0.011018662100637903, ' can': 0.0}
JS List [(' can', 0.0), (' there', 0.011018662100637903), (' was', 0.08302978521071844), (' are', 0.13595678916958887), (' places', 0.15950343635816683)]
highest JS word  places
CURR CONTEXT Where there places JS 0.10947584500799273
NEW WORD  like
CURR PRE-NEW CONTEXT Where there places
NEW CONTEXT Where there places like
{' this': 0.64, ' the': 0.12, ' these': 0.02, ' that': 0.1, ' those': 0.04, ' here': 0.02, ' our': 0.02, " '": 0.02, ' you': 0.02} {' in': 0.04, ' with': 0.02, ' was': 0.02, ' places': 0.08, ' where': 0.36, ' stands': 0.04, ' and': 0.02, ' "': 0.02, ' the': 0.2, ',': 0.04, ' for': 0.02, '.': 0.04, ' (': 0.02, ' is': 0.02, ' a': 0.02, '': 0.02, ' The': 0.02}
NEW WORD  resources
CURR PRE-NEW CONTEXT Where there places
NEW CONTEXT Where there places resources
{' are': 0.18, ' for': 0.14, ' such': 0.02, ' can': 0.02, ' could': 0.02, ' that': 0.08, ' like': 0.06, ' and': 0.02, ',': 0.04, ' to': 0.1, ' on': 0.04, ' needed': 0.02, ' have': 0.02, ' is': 0.02, ' in': 0.08, ' need': 0.02, ' available': 0.04, ' were': 0.02, ' there': 0.02, ' or': 0.02, ' where': 0.02} {'.': 0.16, ' was': 0.04, ',': 0.12, ' the': 0.12, ' resources': 0.32, ' where': 0.02, ' in': 0.04, ' were': 0.04, ' and': 0.02, ' are': 0.02, ' storage': 0.02, ' time': 0.02, ' is': 0.04, ' places': 0.02}
NEW WORD  have
CURR PRE-NEW CONTEXT Where there places
NEW CONTEXT Where there places have
{' been': 0.72, ' no': 0.02, ' to': 0.02, ' a': 0.06, ' gone': 0.06, ' ever': 0.02, ' changed': 0.02, ' never': 0.02, ' come': 0.02, ' always': 0.04} {' been': 0.86, ' the': 0.02, ' since': 0.02, ' taken': 0.02, ' yet': 0.02, ' also': 0.02, ' given': 0.02, ' never': 0.02}
NEW WORD  places
CURR PRE-NEW CONTEXT Where there places
NEW CONTEXT Where there places places
{' to': 0.18, ' are': 0.12, ' no': 0.02, ' that': 0.06, ' in': 0.02, ' you': 0.1, ',': 0.04, ' where': 0.2, ' for': 0.06, ' we': 0.02, ' and': 0.02, ' like': 0.02, ' of': 0.02, ' on': 0.02, ' people': 0.02, ' more': 0.02, ' with': 0.04, ' the': 0.02} {' the': 0.12, ' and': 0.04, ' where': 0.24, ' of': 0.08, ',': 0.06, ' place': 0.12, ' that': 0.02, ' was': 0.02, ' :': 0.02, ' other': 0.02, ' for': 0.04, ' is': 0.02, ' stations': 0.02, ' it': 0.02, ' sites': 0.02, ' locations': 0.02, ' had': 0.02, ' with': 0.02, ' space': 0.02, ' there': 0.04, '.': 0.02}
NEW WORD  are
CURR PRE-NEW CONTEXT Where there places
NEW CONTEXT Where there places are
{' so': 0.06, ' more': 0.12, ' available': 0.04, ' to': 0.02, ' there': 0.08, ' no': 0.12, ' in': 0.02, ' all': 0.02, ' fewer': 0.02, ' for': 0.04, ' like': 0.04, ' open': 0.06, ',': 0.06, ' too': 0.02, ' some': 0.02, ' people': 0.04, ' a': 0.02, ' going': 0.02, ' such': 0.02, ' only': 0.02, ' where': 0.04, ' that': 0.02, ' not': 0.04, ' two': 0.02, ' the': 0.02} {' places': 0.46, ' is': 0.02, ' many': 0.06, ' where': 0.02, ' areas': 0.02, ',': 0.08, ' there': 0.02, ' to': 0.02, ' the': 0.08, ' stands': 0.02, ' with': 0.04, '.': 0.02, ' locations': 0.02, ' in': 0.06, ' and': 0.04, ' two': 0.02}
{' like': 0.0, ' resources': 0.1554666229213416, ' have': 9.538184852947982e-05, ' places': 0.06668796722940602, ' are': 0.09381367064084645}
JS List [(' like', 0.0), (' have', 9.538184852947982e-05), (' places', 0.06668796722940602), (' are', 0.09381367064084645), (' resources', 0.1554666229213416)]
highest JS word  resources
CURR CONTEXT Where there places resources JS 0.36387935117841924
NEW WORD  like
CURR PRE-NEW CONTEXT Where there places resources
NEW CONTEXT Where there places resources like
{' our': 0.02, ' this': 0.42, ' that': 0.08, ' the': 0.08, ' your': 0.02, ' Amazon': 0.02, ' Kickstarter': 0.02, ' these': 0.04, ' health': 0.04, ' Google': 0.04, ' a': 0.04, ' education': 0.02, ' Wikipedia': 0.02, ' those': 0.04, ' my': 0.02, ' food': 0.06, ' it': 0.02} {' storage': 0.04, ' reeds': 0.02, ' resources': 0.24, ' the': 0.26, ' those': 0.02, ' water': 0.1, ' stores': 0.04, ' places': 0.02, ' reserves': 0.04, ' supplies': 0.02, ' firewood': 0.02, ' rivers': 0.02, '': 0.04, ' ammunition': 0.04, ' facilities': 0.02, ' stocks': 0.02, ' ships': 0.02, ' food': 0.02}
NEW WORD  resources
CURR PRE-NEW CONTEXT Where there places resources
NEW CONTEXT Where there places resources resources
{' on': 0.02, ' (': 0.02, ',': 0.06, ' resource': 0.06, ' the': 0.06, ' will': 0.02, ' are': 0.14, ' that': 0.08, '-': 0.02, '/': 0.02, ' to': 0.14, ' from': 0.02, ' can': 0.06, ':': 0.04, ' have': 0.04, '.': 0.02, ' as': 0.02, ' and': 0.04, ' is': 0.02, ' for': 0.02, ' where': 0.02, ' you': 0.02, ' would': 0.02, ' not': 0.02} {' there': 0.06, ',': 0.2, ' and': 0.08, ' available': 0.06, '.': 0.12, ' were': 0.02, ' they': 0.02, ' for': 0.06, ' (': 0.02, ' places': 0.06, ' supplies': 0.02, ' the': 0.1, ' it': 0.04, ' is': 0.02, ' reserves': 0.04, ' which': 0.02, ' storage': 0.02, ' to': 0.02, ' not': 0.02}
NEW WORD  places
CURR PRE-NEW CONTEXT Where there places resources
NEW CONTEXT Where there places resources places
{' to': 0.04, ' an': 0.04, ',': 0.06, ' a': 0.08, ' people': 0.2, ' it': 0.04, ' the': 0.12, ' life': 0.02, ' in': 0.04, ' your': 0.04, ' where': 0.02, ' you': 0.04, ' them': 0.04, ' for': 0.02, ' money': 0.02, '.': 0.02, ' great': 0.02, ' and': 0.02, ' are': 0.02, ' things': 0.02, ' us': 0.02, ' more': 0.02, ' other': 0.04} {' the': 0.34, ',': 0.06, ' a': 0.04, ' with': 0.04, ' places': 0.06, ' priority': 0.02, ' in': 0.04, ' of': 0.04, ' where': 0.02, ' facilities': 0.02, '.': 0.06, ' resource': 0.02, ' that': 0.02, ' most': 0.04, ' those': 0.06, ' it': 0.02, ' ground': 0.02, ' time': 0.02, ' place': 0.04, ' all': 0.02}
NEW WORD  are
CURR PRE-NEW CONTEXT Where there places resources
NEW CONTEXT Where there places resources are
{' available': 0.34, ' lacking': 0.02, ' at': 0.04, ' plentiful': 0.02, ' being': 0.06, ' there': 0.02, ' scarce': 0.12, ' not': 0.1, ' needed': 0.08, ' less': 0.06, ' more': 0.04, ' located': 0.04, ' left': 0.02, ' used': 0.04} {' resources': 0.28, ',': 0.24, ' the': 0.24, ' not': 0.02, ' places': 0.04, ' needed': 0.04, '.': 0.04, ' in': 0.02, ' was': 0.02, ' where': 0.02, ' water': 0.02, ' most': 0.02}
NEW WORD  was
CURR PRE-NEW CONTEXT Where there places resources
NEW CONTEXT Where there places resources was
{' needed': 0.06, ' used': 0.02, ' provided': 0.08, ' available': 0.14, ' in': 0.04, ' a': 0.06, ' not': 0.2, ' never': 0.02, ' there': 0.04, ' found': 0.04, ' limited': 0.02, ' the': 0.02, ' so': 0.02, ' less': 0.06, ',': 0.06, ' on': 0.02, ' scarce': 0.02, ' taken': 0.02, ' allocated': 0.02, ' lacking': 0.02, ' created': 0.02} {'.': 0.16, ' and': 0.02, ' set': 0.02, ',': 0.1, ' where': 0.08, ' a': 0.14, ' the': 0.18, ' in': 0.04, ' "': 0.02, ' found': 0.02, ' space': 0.02, ' :': 0.04, ' next': 0.02, ' for': 0.02, ' again': 0.02, ' available': 0.02, ' only': 0.02, ' better': 0.02, ' more': 0.02, ' not': 0.02}
{' like': 0.1052147902160554, ' resources': 0.1212434894964372, ' places': 0.058682927795070894, ' are': 0.02522370415125534, ' was': 0.21713144792420955}
JS List [(' are', 0.02522370415125534), (' places', 0.058682927795070894), (' like', 0.1052147902160554), (' resources', 0.1212434894964372), (' was', 0.21713144792420955)]
highest JS word  was
CURR CONTEXT Where there places resources was JS 0.5693617597009333
2020-09-07 13:30:46.705389: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:30:46.705501: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:30:46.705510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-09-07 13:30:51.959222: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-09-07 13:30:51.994009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-07 13:30:51.997631: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-09-07 13:30:51.997680: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: node082
2020-09-07 13:30:51.997687: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: node082
2020-09-07 13:30:51.997795: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.82.0
2020-09-07 13:30:51.997832: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.82.0
2020-09-07 13:30:51.997838: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.82.0
2020-09-07 13:30:51.998194: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-09-07 13:30:52.006809: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-09-07 13:30:52.007669: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e94afaf320 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-07 13:30:52.007701: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
All model checkpoint weights were used when initializing TFGPT2LMHeadModel.

All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
All model checkpoint weights were used when initializing TFTransfoXLLMHeadModel.

All the weights of TFTransfoXLLMHeadModel were initialized from the model checkpoint at transfo-xl-wt103.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFTransfoXLLMHeadModel for predictions without further training.
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
NEW WORD 
CURR PRE-NEW CONTEXT When
NEW CONTEXT When
{' the': 0.18, '\n': 0.04, "'s": 0.04, ' people': 0.04, ',': 0.04, ' it': 0.08, ' your': 0.02, ' that': 0.02, ' of': 0.02, ' he': 0.04, ' a': 0.08, ' his': 0.02, ' is': 0.04, ' I': 0.08, ' to': 0.04, ' you': 0.06, ' there': 0.02, '-': 0.02, ' an': 0.04, ' we': 0.02, ' from': 0.02, ' some': 0.02, ' my': 0.02} {' then': 0.06, ' Knopf': 0.04, ' a': 0.1, ' he': 0.02, ',': 0.1, ' time': 0.04, ' the': 0.14, ' asked': 0.16, ' one': 0.02, ' first': 0.02, ' when': 0.02, ' she': 0.08, '': 0.02, ' an': 0.04, ' it': 0.04, ' was': 0.04, ' healed': 0.02, ' irritated': 0.02, ' with': 0.02}
NEW WORD  and
CURR PRE-NEW CONTEXT When
NEW CONTEXT When and
{' where': 0.2, ' if': 0.12, ' why': 0.18, ' how': 0.32, ' when': 0.14, ' Why': 0.02, ' Where': 0.02} {' when': 0.16, ' Millett': 0.16, ' for': 0.02, ' was': 0.04, ' a': 0.04, ' first': 0.1, ' Knopf': 0.02, ',': 0.28, ' with': 0.02, ' (': 0.02, ' and': 0.04, ' or': 0.02, ' after': 0.02, ' her': 0.02, ' the': 0.02, ' strike': 0.02}
NEW WORD  we
CURR PRE-NEW CONTEXT When
NEW CONTEXT When we
{' talk': 0.1, ' went': 0.02, "'re": 0.1, ' begin': 0.04, ' say': 0.04, ' look': 0.06, ' were': 0.1, ' don': 0.02, ' got': 0.02, ' see': 0.04, ' asked': 0.02, ' speak': 0.02, ' talked': 0.02, ' do': 0.04, ' did': 0.02, ' are': 0.02, ' had': 0.06, ' think': 0.04, ' get': 0.02, ' go': 0.04, ' looked': 0.02, ' came': 0.02, ' know': 0.04, ' have': 0.04, ' saw': 0.02, ' make': 0.02} {' asked': 0.3, ' first': 0.12, ' and': 0.04, '': 0.02, ' were': 0.16, ',': 0.06, ' when': 0.02, ' people': 0.02, ' see': 0.02, ' in': 0.04, ' came': 0.02, ' the': 0.02, " '": 0.02, ' they': 0.02, ' a': 0.02, ' then': 0.02, ' we': 0.02, ' at': 0.02, ' gaze': 0.02, ' call': 0.02}
NEW WORD  first
CURR PRE-NEW CONTEXT When
NEW CONTEXT When first
{' getting': 0.02, ' appearing': 0.04, ' it': 0.02, ' elected': 0.02, ',': 0.1, ' spotted': 0.02, ' reading': 0.04, ' I': 0.2, ' coming': 0.02, ' he': 0.02, ' started': 0.04, ' being': 0.02, ' starting': 0.04, ' hearing': 0.06, ' introduced': 0.04, ' they': 0.02, ' installed': 0.04, ' launched': 0.06, ' brought': 0.02, ' posted': 0.04, ' she': 0.02, ' heard': 0.02, ' announced': 0.02, ' put': 0.02, ' released': 0.02, ' seeing': 0.02} {' first': 0.26, ',': 0.28, ' saw': 0.04, ' awakened': 0.06, ' was': 0.08, ' (': 0.02, ' with': 0.02, ' came': 0.04, ' @-@': 0.02, ' went': 0.04, ' became': 0.02, ' asked': 0.02, ' broke': 0.02, ' and': 0.02, ' it': 0.02, ' he': 0.02, ' the': 0.02}
NEW WORD  (
CURR PRE-NEW CONTEXT When
NEW CONTEXT When (
{'size': 0.04, 'key': 0.02, 'empty': 0.04, '1': 0.06, 'i': 0.08, '!': 0.04, 'e': 0.06, 'item': 0.04, 'Is': 0.02, 'x': 0.02, 'n': 0.04, 'un': 0.06, 'c': 0.02, 'current': 0.02, 'string': 0.06, 'str': 0.06, ' )': 0.04, 'a': 0.02, 'length': 0.02, 'P': 0.02, 'not': 0.04, 'int': 0.02, 'is': 0.02, 'null': 0.02, 'this': 0.02, 'd': 0.02, 'true': 0.04, 'f': 0.02, '\n': 0.02} {' )': 0.62, ' "': 0.02, ' or': 0.16, ' its': 0.02, ',': 0.08, ' a': 0.02, ' happens': 0.04, ' (': 0.02, ' as': 0.02}
{'': 0.029842756097666203, ' and': 0.0, ' we': 0.15198350211954062, ' first': 0.02022003322665032, ' (': 0.0}
JS List [(' and', 0.0), (' (', 0.0), (' first', 0.02022003322665032), ('', 0.029842756097666203), (' we', 0.15198350211954062)]
highest JS word  we
CURR CONTEXT When we JS 0.13258794276943914
NEW WORD  came
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we came
{' to': 0.32, ' down': 0.04, ' up': 0.16, ' back': 0.18, ' into': 0.04, ' across': 0.1, ' out': 0.06, ' in': 0.02, ' here': 0.04, ' from': 0.02, ' over': 0.02} {' to': 0.28, ' close': 0.02, ',': 0.34, ' in': 0.06, ' and': 0.04, ' our': 0.06, ' we': 0.04, ' on': 0.04, ' near': 0.02, ' a': 0.02, ' of': 0.02, ' out': 0.02, ' first': 0.02, ' came': 0.02}
NEW WORD  first
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we first
{' met': 0.1, ' saw': 0.1, ' heard': 0.12, ' began': 0.04, ' came': 0.1, ' got': 0.04, ' started': 0.18, ' looked': 0.04, ' moved': 0.02, ' made': 0.02, ' meet': 0.06, ' tried': 0.02, ' approached': 0.02, ' spoke': 0.04, ' took': 0.04, ' launched': 0.02, ' went': 0.02, ' learned': 0.02} {',': 0.14, ' were': 0.02, ' we': 0.16, ' saw': 0.1, ' our': 0.02, ' see': 0.02, ' first': 0.06, ' contacted': 0.04, ' the': 0.1, ' when': 0.02, ' was': 0.02, ' heard': 0.06, ' knew': 0.02, ' a': 0.02, ' learned': 0.02, ' and': 0.02, ' at': 0.02, ' came': 0.04, ' think': 0.02, " '": 0.02, ' he': 0.04, ' they': 0.02}
NEW WORD  we
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we we
{' go': 0.06, "'re": 0.08, ' saw': 0.04, ' put': 0.02, ' use': 0.02, ' have': 0.08, ' start': 0.02, ' talk': 0.06, ' think': 0.04, ' had': 0.02, ' are': 0.04, ' make': 0.02, ' moved': 0.02, ' all': 0.02, ' began': 0.02, ' were': 0.08, ' got': 0.06, ' started': 0.02, ' do': 0.06, ' get': 0.04, "'ve": 0.04, ' move': 0.04, ' see': 0.04, ' come': 0.02, ' take': 0.02, ' say': 0.02} {' ourselves': 0.06, ' people': 0.04, ' knew': 0.02, ' with': 0.02, ' they': 0.02, ' when': 0.02, ',': 0.22, ' then': 0.02, ' the': 0.06, ' first': 0.02, ' gaze': 0.08, " '": 0.02, ' asked': 0.06, ' heard': 0.04, ' were': 0.06, ' are': 0.02, ' came': 0.02, ' our': 0.02, ' told': 0.02, ' had': 0.02, ' saw': 0.04, ' could': 0.02, ' all': 0.02, ' see': 0.06}
NEW WORD 
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we
{' started': 0.06, ' saw': 0.04, ' use': 0.02, ' began': 0.02, ' hear': 0.02, ' can': 0.02, ' went': 0.02, ' say': 0.06, ' looked': 0.04, ' talk': 0.06, ' have': 0.02, ' first': 0.02, ' said': 0.04, "'re": 0.16, ' did': 0.04, ' were': 0.06, ' come': 0.02, ' do': 0.02, ' look': 0.08, ' had': 0.04, ' start': 0.02, ' put': 0.04, ' go': 0.02, ' came': 0.02, ' take': 0.02, ' get': 0.02} {' we': 0.04, ' the': 0.1, ' first': 0.1, ' when': 0.02, ' at': 0.02, ' see': 0.04, ' asked': 0.14, ' learned': 0.02, " '": 0.02, ' were': 0.04, ',': 0.08, ' heard': 0.02, ' was': 0.02, ' people': 0.02, ' it': 0.04, ' a': 0.04, ' saw': 0.12, ' call': 0.02, ' had': 0.02, ' they': 0.02, ' think': 0.02, ' [': 0.02, ' came': 0.02}
NEW WORD  see
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we see
{' this': 0.08, ' those': 0.06, ' him': 0.04, ' the': 0.14, ' them': 0.08, ' any': 0.02, ' that': 0.08, ' you': 0.04, ' what': 0.02, ' a': 0.08, ' these': 0.04, ' something': 0.08, ' it': 0.04, ' an': 0.06, ' one': 0.02, ' people': 0.02, ' kids': 0.04, ' our': 0.02, ' more': 0.02, ',': 0.02} {' you': 0.02, ' it': 0.14, ',': 0.12, ' we': 0.04, ' see': 0.12, ' they': 0.02, ' our': 0.04, ' the': 0.2, ' a': 0.08, ' how': 0.04, ' this': 0.02, ' gaze': 0.04, ' he': 0.02, ' someone': 0.02, ' in': 0.02, ' [': 0.02, ' of': 0.04}
{' came': 0.02791354905305543, ' first': 0.019658964995545175, ' we': 0.013964662800034423, '': 0.088983761428613, ' see': 0.08025236275763467}
JS List [(' we', 0.013964662800034423), (' first', 0.019658964995545175), (' came', 0.02791354905305543), (' see', 0.08025236275763467), ('', 0.088983761428613)]
highest JS word 
CURR CONTEXT When we JS 0.23351233103026198
NEW WORD  came
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we came
{' to': 0.44, ' off': 0.02, ' up': 0.1, ' back': 0.12, ' here': 0.08, ' across': 0.02, ' out': 0.1, ' from': 0.02, ' into': 0.06, ' home': 0.02, ' down': 0.02} {',': 0.4, ' next': 0.04, ' we': 0.08, ' to': 0.1, ' from': 0.02, ' into': 0.04, ' the': 0.04, ' on': 0.04, ' home': 0.02, ' it': 0.02, ' of': 0.04, ' as': 0.02, ' through': 0.04, ' with': 0.02, ' out': 0.02, ' at': 0.04, ' he': 0.02}
NEW WORD  first
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we first
{' played': 0.06, ' started': 0.12, ' began': 0.06, ' looked': 0.04, ' saw': 0.06, ' announced': 0.02, ' met': 0.14, ' got': 0.12, ' introduced': 0.02, ' moved': 0.02, ' get': 0.02, ' heard': 0.04, ' read': 0.04, ' arrived': 0.02, ' had': 0.04, ' meet': 0.02, ' tried': 0.02, ' entered': 0.04, ' set': 0.04, ' came': 0.02, ' encountered': 0.04} {' we': 0.1, ' were': 0.02, ' [': 0.02, ' saw': 0.12, ' got': 0.04, ' when': 0.02, ',': 0.24, ' heard': 0.04, ' went': 0.04, ' contacted': 0.02, ' the': 0.04, ' you': 0.04, ' a': 0.02, ' our': 0.04, ' and': 0.02, ' people': 0.02, ' came': 0.02, ' first': 0.04, ' asked': 0.02, ' see': 0.02, ' think': 0.02, ' had': 0.02, ' know': 0.02}
NEW WORD  we
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we we
{' have': 0.06, ' look': 0.04, ' found': 0.02, ' meet': 0.06, ' were': 0.1, ' do': 0.06, ' say': 0.02, "'re": 0.02, ' all': 0.02, ' got': 0.02, ' are': 0.08, ' can': 0.08, ' talk': 0.04, "'ve": 0.04, ' started': 0.02, ' make': 0.02, ' get': 0.04, ' see': 0.06, ' had': 0.04, ' think': 0.02, ' start': 0.04, ' go': 0.02, ' begin': 0.02, ' looked': 0.02, ' don': 0.04} {' are': 0.04, ' were': 0.12, ' [': 0.02, ' think': 0.04, ' a': 0.04, ' first': 0.1, ' see': 0.04, ' the': 0.12, ' then': 0.04, ' it': 0.04, ' gaze': 0.02, " '": 0.02, ' when': 0.02, ',': 0.16, ' in': 0.04, ' as': 0.02, ' told': 0.02, ' did': 0.02, ' asked': 0.02, ' all': 0.02, ' ourselves': 0.02, ' know': 0.02}
NEW WORD  see
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we see
{' these': 0.04, ' this': 0.08, ' a': 0.22, ' them': 0.04, ' it': 0.04, ' those': 0.08, ' the': 0.16, ' two': 0.02, ' that': 0.1, ' other': 0.02, ' all': 0.02, ' him': 0.02, ' your': 0.02, ' people': 0.04, ' how': 0.02, ' her': 0.02, ',': 0.02, ' their': 0.02, ' such': 0.02} {' that': 0.06, ' it': 0.16, ' the': 0.26, ' see': 0.02, ' you': 0.02, ' we': 0.06, ',': 0.1, ' someone': 0.06, ' and': 0.02, ' there': 0.02, ' a': 0.06, ' saw': 0.02, ' of': 0.04, ' what': 0.02, ' at': 0.02, ' his': 0.02, ' they': 0.02, ' when': 0.02}
NEW WORD  were
CURR PRE-NEW CONTEXT When we
NEW CONTEXT When we were
{' looking': 0.04, ' all': 0.06, ' at': 0.14, ' doing': 0.02, ' living': 0.02, ' running': 0.02, ' working': 0.04, ' making': 0.02, ' done': 0.04, ' getting': 0.08, ' talking': 0.06, ' young': 0.04, ' about': 0.02, ' first': 0.04, ' sitting': 0.04, ' in': 0.12, ' growing': 0.04, ' watching': 0.02, ' playing': 0.02, ' able': 0.02, ' still': 0.02, ' there': 0.02, ' told': 0.02, ' younger': 0.02, ' starting': 0.02} {' we': 0.02, ' first': 0.02, ' asked': 0.22, ',': 0.08, ' a': 0.04, ' in': 0.12, ' on': 0.02, ' were': 0.04, ' the': 0.1, ' many': 0.04, ' still': 0.04, ' not': 0.04, ' members': 0.02, ' and': 0.02, ' there': 0.02, ' at': 0.02, '': 0.02, ' early': 0.02, ' teenagers': 0.02, ' as': 0.02, ' trying': 0.04, ' awakened': 0.02}
{' came': 0.03930936249941639, ' first': 0.06385637625208895, ' we': 0.02364014758935114, ' see': 0.10986240392297361, ' were': 0.08488013557059457}
JS List [(' we', 0.02364014758935114), (' came', 0.03930936249941639), (' first', 0.06385637625208895), (' were', 0.08488013557059457), (' see', 0.10986240392297361)]
highest JS word  see
CURR CONTEXT When we see JS 0.2640737649657826
NEW WORD  we
CURR PRE-NEW CONTEXT When we see
NEW CONTEXT When we see we
{' are': 0.08, "'re": 0.32, ' have': 0.28, ' were': 0.02, "'ve": 0.1, ' should': 0.02, ' can': 0.06, ' could': 0.02, ' did': 0.04, ' don': 0.02, ' get': 0.02, ' go': 0.02} {' look': 0.16, ' he': 0.02, ',': 0.1, ' watch': 0.02, ' hear': 0.14, ' we': 0.02, ' it': 0.02, ' are': 0.04, ' know': 0.06, ' the': 0.04, ' they': 0.02, ' saw': 0.06, ' find': 0.04, ' at': 0.02, ' in': 0.02, ' go': 0.02, ' do': 0.04, ' understand': 0.02, ' think': 0.04, ' listen': 0.02, ' feel': 0.02, ' realize': 0.02, ' sees': 0.02, ' get': 0.02}
NEW WORD  see
CURR PRE-NEW CONTEXT When we see
NEW CONTEXT When we see see
{' this': 0.16, ' that': 0.16, ' more': 0.02, ' something': 0.04, ' how': 0.02, ' it': 0.06, ' a': 0.16, ' someone': 0.02, ' these': 0.06, ' the': 0.1, ' him': 0.04, ' them': 0.06, ' what': 0.02, ' somebody': 0.02, ' people': 0.02, ' her': 0.02, ' an': 0.02} {' how': 0.06, ' the': 0.32, " '": 0.04, ' look': 0.02, ',': 0.08, ' as': 0.02, ' view': 0.02, ' gaze': 0.02, ' of': 0.02, ' sees': 0.02, ' it': 0.2, ' in': 0.02, ' glimpse': 0.02, ' what': 0.02, ' a': 0.02, "'s": 0.02, ' you': 0.02, ' saw': 0.02, ' his': 0.02, ' at': 0.02}
NEW WORD  a
CURR PRE-NEW CONTEXT When we see
NEW CONTEXT When we see a
{' woman': 0.02, ' couple': 0.04, ' lot': 0.2, ' red': 0.04, ' single': 0.04, ' person': 0.04, ' guy': 0.16, ' movie': 0.02, ' number': 0.02, ' few': 0.02, ' new': 0.1, ' child': 0.02, ' black': 0.06, ' white': 0.02, ' player': 0.02, ' good': 0.06, ' girl': 0.02, ' problem': 0.02, ' man': 0.02, ' very': 0.02, ' car': 0.04} {' look': 0.04, ' glimpse': 0.22, ' shot': 0.06, ' picture': 0.08, ' one': 0.04, ' he': 0.06, ',': 0.08, ' a': 0.02, ' saw': 0.04, ' "': 0.02, ' film': 0.04, ' CG': 0.02, ' scene': 0.04, ' the': 0.12, " '": 0.04, ' vision': 0.04, ' light': 0.02, ' man': 0.02}
NEW WORD  all
CURR PRE-NEW CONTEXT When we see
NEW CONTEXT When we see all
{' of': 0.2, ' the': 0.2, ' these': 0.36, ' this': 0.12, ' those': 0.08, ' that': 0.04} {',': 0.2, ' of': 0.34, ' we': 0.04, ' the': 0.24, ' things': 0.02, ' in': 0.1, ' @-@': 0.02, ' see': 0.02, ' they': 0.02}
NEW WORD  someone
CURR PRE-NEW CONTEXT When we see
NEW CONTEXT When we see someone
{' on': 0.1, ' with': 0.14, ' who': 0.16, ' get': 0.04, ' holding': 0.04, ' like': 0.06, ' in': 0.12, ' take': 0.04, ' getting': 0.04, ' and': 0.02, ' coming': 0.02, "'s": 0.06, ' doing': 0.04, ' wearing': 0.02, ' go': 0.04, ' being': 0.02, ' trying': 0.02, ' die': 0.02} {' a': 0.02, ' someone': 0.08, ' to': 0.02, ' and': 0.06, ' else': 0.04, ' you': 0.02, ' it': 0.02, ',': 0.38, ' at': 0.02, ' who': 0.08, ' in': 0.06, ' he': 0.04, ' the': 0.06, ' other': 0.02, ' they': 0.02, ' of': 0.02, ' (': 0.02, ' she': 0.02}
{' we': 0.014362591564146626, ' see': 0.14140146119644625, ' a': 0.0, ' all': 0.0037484626598303337, ' someone': 0.048513354327289974}
JS List [(' a', 0.0), (' all', 0.0037484626598303337), (' we', 0.014362591564146626), (' someone', 0.048513354327289974), (' see', 0.14140146119644625)]
highest JS word  see
CURR CONTEXT When we see see JS 0.39425843174211106
2020-09-07 13:43:59.775545: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:43:59.775670: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:43:59.775683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-09-07 13:44:08.765503: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-09-07 13:44:08.849679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-07 13:44:08.871066: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-09-07 13:44:08.871136: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: node082
2020-09-07 13:44:08.871148: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: node082
2020-09-07 13:44:08.871305: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.82.0
2020-09-07 13:44:08.871358: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.82.0
2020-09-07 13:44:08.871368: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.82.0
2020-09-07 13:44:08.871863: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-09-07 13:44:08.884641: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-09-07 13:44:08.885558: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5561b71aa700 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-07 13:44:08.885597: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
All model checkpoint weights were used when initializing TFGPT2LMHeadModel.

All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
All model checkpoint weights were used when initializing TFTransfoXLLMHeadModel.

All the weights of TFTransfoXLLMHeadModel were initialized from the model checkpoint at transfo-xl-wt103.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFTransfoXLLMHeadModel for predictions without further training.
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
NEW WORD  to
CURR PRE-NEW CONTEXT Why
NEW CONTEXT Why to
{' put': 0.08, ' learn': 0.02, ' Use': 0.02, ' read': 0.06, ' write': 0.04, ' watch': 0.06, ' choose': 0.02, ' Do': 0.08, ' Apply': 0.02, ' go': 0.04, ' Know': 0.04, ' Buy': 0.02, ' get': 0.12, ' use': 0.06, ' tell': 0.02, ' Become': 0.02, ' do': 0.06, ' Start': 0.02, ' Make': 0.04, ' make': 0.1, ' become': 0.02, ' Play': 0.02, ' find': 0.02} {' have': 0.02, '': 0.02, ' us': 0.02, ' Win': 0.02, ' where': 0.02, ' so': 0.3, ' for': 0.02, ' when': 0.02, ' do': 0.02, ' the': 0.04, ' not': 0.02, ' "': 0.04, ' Not': 0.02, ' make': 0.02, ' I': 0.02, ' our': 0.02, '?': 0.18, ' at': 0.02, ' Be': 0.02, ' here': 0.02, ' to': 0.02, ' his': 0.02, ' this': 0.02, ' be': 0.02, ' :': 0.02, ',': 0.02}
NEW WORD  on
CURR PRE-NEW CONTEXT Why
NEW CONTEXT Why on
{' Earth': 0.26, ' earth': 0.74} {'?': 0.66, ' earth': 0.02, ' the': 0.1, '': 0.06, ' when': 0.02, ' so': 0.06, ' this': 0.02, ' at': 0.02, ',': 0.02, ' Earth': 0.02}
NEW WORD  I
CURR PRE-NEW CONTEXT Why
NEW CONTEXT Why I
{' was': 0.08, ' have': 0.02, ' didn': 0.02, ' say': 0.02, ' am': 0.12, ' said': 0.02, ' should': 0.04, ' got': 0.02, "'ve": 0.06, ' think': 0.08, ' would': 0.02, ' could': 0.02, ' know': 0.02, "'m": 0.16, ' can': 0.06, ' believe': 0.04, ' Can': 0.02, ' went': 0.02, ' like': 0.04, ' Love': 0.02, ' saw': 0.04, ' love': 0.02, ' used': 0.02, ' wanted': 0.02} {'?': 0.34, ' was': 0.06, ' why': 0.02, ' myself': 0.02, ' for': 0.02, ' who': 0.02, ' the': 0.02, ' and': 0.1, '': 0.06, ' I': 0.1, ' when': 0.02, ' Why': 0.02, ' did': 0.02, ' as': 0.02, ',': 0.08, ' is': 0.02, ' not': 0.04, '.': 0.02}
NEW WORD  this
CURR PRE-NEW CONTEXT Why
NEW CONTEXT Why this
{' site': 0.04, ' is': 0.42, ' story': 0.04, ' year': 0.02, ',': 0.04, ' happened': 0.02, ' was': 0.1, ' one': 0.02, ' does': 0.02, ' article': 0.02, ' particular': 0.02, ' book': 0.02, ' guy': 0.02, ' makes': 0.02, ' works': 0.02, ' isn': 0.04, ' can': 0.04, ' doesn': 0.02, ' has': 0.02, ' will': 0.02, ' could': 0.02} {' so': 0.56, ' such': 0.08, ' it': 0.04, ' is': 0.1, '?': 0.02, ' and': 0.02, ' the': 0.02, ',': 0.04, ' that': 0.06, ' phenomenon': 0.02, ' violates': 0.02, ' "': 0.02}
NEW WORD  were
CURR PRE-NEW CONTEXT Why
NEW CONTEXT Why were
{' you': 0.32, ' we': 0.16, ' those': 0.04, ' she': 0.02, ' the': 0.22, ' your': 0.04, ' these': 0.04, ' his': 0.02, ' so': 0.02, ' they': 0.08, ' it': 0.02, ' all': 0.02} {'?': 0.86, ',': 0.04, ' so': 0.06, ' such': 0.02, ' and': 0.02}
{' to': 0.007958997917494562, ' on': 0.030962077718425206, ' I': 0.0, ' this': 0.0340110257544377, ' were': 0.0}
JS List [(' I', 0.0), (' were', 0.0), (' to', 0.007958997917494562), (' on', 0.030962077718425206), (' this', 0.0340110257544377)]
highest JS word  this
CURR CONTEXT Why this JS 0.013905425335135124
NEW WORD  process
CURR PRE-NEW CONTEXT Why this
NEW CONTEXT Why this process
{' of': 0.06, ' is': 0.44, ' would': 0.06, ' has': 0.04, ' could': 0.04, ' starts': 0.02, ' will': 0.04, ' seems': 0.02, ' had': 0.02, ' works': 0.06, ' was': 0.04, ' can': 0.04, ' also': 0.02, ',': 0.02, ' began': 0.02, ' for': 0.02, ' should': 0.02, ' continues': 0.02} {' so': 0.12, ' was': 0.24, ' is': 0.32, ' and': 0.04, ',': 0.02, ' not': 0.02, ' happens': 0.02, ' that': 0.04, ' its': 0.02, ' cannot': 0.02, ' "': 0.02, '?': 0.04, ' the': 0.02, ' works': 0.02, ' such': 0.02, ' violates': 0.02}
NEW WORD ?
CURR PRE-NEW CONTEXT Why this
NEW CONTEXT Why this?
{'\n': 0.22, ' Who': 0.02, ' Why': 0.08, ' A': 0.1, ' In': 0.02, ' It': 0.08, ' Is': 0.04, ' What': 0.04, ' Because': 0.06, ' I': 0.02, ' This': 0.06, ' "': 0.02, ' We': 0.04, ' He': 0.02, ' Did': 0.02, ' Well': 0.02, ' The': 0.08, ' That': 0.02, ' If': 0.02, ' How': 0.02} {'Why,': 0.12, 'Why so': 0.16, 'Why?': 0.3, 'Why the': 0.02, "Why's": 0.04, 'Why is': 0.06, 'Why was': 0.02, "Why 't": 0.02, 'Why': 0.16, 'Why and': 0.04, 'Why such': 0.02, 'Why.': 0.02, 'Why in': 0.02}
NEW WORD  story
CURR PRE-NEW CONTEXT Why this
NEW CONTEXT Why this story
{' really': 0.02, ' is': 0.28, ' about': 0.04, ' has': 0.12, ' does': 0.02, ' ends': 0.02, ' and': 0.02, ' may': 0.02, ' seems': 0.02, ' was': 0.08, ' will': 0.08, ' came': 0.02, ' doesn': 0.06, ' gets': 0.02, ' wasn': 0.02, ',': 0.02, ' goes': 0.04, ' begins': 0.02, ' makes': 0.02, ' needs': 0.02, ' comes': 0.02, ' should': 0.02} {' is': 0.3, ' (': 0.02, ' was': 0.18, '?': 0.06, ',': 0.06, ' this': 0.02, ' so': 0.12, ' the': 0.08, ' since': 0.02, ' has': 0.02, ' it': 0.02, ' in': 0.04, ' today': 0.02, ' for': 0.02, ' remains': 0.02}
NEW WORD  happens
CURR PRE-NEW CONTEXT Why this
NEW CONTEXT Why this happens
{' to': 0.12, ' and': 0.02, ' is': 0.36, '?': 0.1, ':': 0.04, ' with': 0.02, ',': 0.16, ' in': 0.1, ' isn': 0.02, ',"': 0.02, ' on': 0.02, ' all': 0.02} {',': 0.24, ' such': 0.02, ' is': 0.36, ' :': 0.02, ' has': 0.02, ' in': 0.02, ' happens': 0.02, ' so': 0.08, ' where': 0.02, '.': 0.04, ' it': 0.02, ' the': 0.02, ' this': 0.02, ' and': 0.04, ' causes': 0.02, '': 0.02, ' as': 0.02}
NEW WORD  this
CURR PRE-NEW CONTEXT Why this
NEW CONTEXT Why this this
{' isn': 0.04, ' time': 0.06, ' new': 0.04, ' is': 0.44, ' should': 0.04, ' guy': 0.02, ' post': 0.04, ' all': 0.02, ' means': 0.02, ' thing': 0.02, ' could': 0.02, ' book': 0.02, ' week': 0.02, ' "': 0.02, ' was': 0.04, ' works': 0.02, ' can': 0.02, ' article': 0.04, ' system': 0.02, ' happened': 0.02, ',': 0.02} {' is': 0.3, ' was': 0.24, ' so': 0.08, ' the': 0.02, ',': 0.08, ' caused': 0.06, ' makes': 0.02, ' today': 0.04, ' "': 0.02, ' does': 0.02, ' and': 0.02, ' its': 0.02, '?': 0.02, ' such': 0.02, ' shows': 0.02, ' happened': 0.02}
{' process': 0.08549591278254182, '?': 0.0, ' story': 0.06429427167113486, ' happens': 0.031012766452116227, ' this': 0.09024020589927975}
JS List [('?', 0.0), (' happens', 0.031012766452116227), (' story', 0.06429427167113486), (' process', 0.08549591278254182), (' this', 0.09024020589927975)]
highest JS word  this
CURR CONTEXT Why this this JS 0.06369035104665771
NEW WORD ?
CURR PRE-NEW CONTEXT Why this this
NEW CONTEXT Why this this?
{'\n': 0.2, ' It': 0.08, ' Why': 0.06, ' Well': 0.06, ' Is': 0.04, ' There': 0.02, ' Because': 0.02, ' No': 0.04, ' The': 0.06, ' That': 0.04, ' This': 0.06, ' How': 0.04, ' He': 0.02, ' Where': 0.02, ' In': 0.02, '\n\n': 0.02, ' Oh': 0.02, ' What': 0.06, ' I': 0.04, ' Does': 0.02, ' And': 0.04, ' Do': 0.02} {'Why this :': 0.02, 'Why this was': 0.1, 'Why this?': 0.08, 'Why this so': 0.18, 'Why this its': 0.06, 'Why this,': 0.16, 'Why this "': 0.02, 'Why this is': 0.08, 'Why this has': 0.06, 'Why this made': 0.02, 'Why this the': 0.02, 'Why this': 0.02, 'Why this in': 0.04, 'Why this this': 0.02, 'Why this @-@': 0.02, 'Why this violates': 0.02, 'Why this causes': 0.02, 'Why this caused': 0.02, 'Why this.': 0.02, 'Why this such': 0.02}
NEW WORD  story
CURR PRE-NEW CONTEXT Why this this
NEW CONTEXT Why this this story
{' isn': 0.02, ' was': 0.02, ' is': 0.5, ' may': 0.02, ' began': 0.02, ',': 0.06, ' doesn': 0.06, ' could': 0.04, ' should': 0.02, ' about': 0.04, ' relates': 0.02, ' can': 0.02, ' has': 0.06, ' matters': 0.04, ' came': 0.02, ' started': 0.02, ' does': 0.02} {' is': 0.36, ' so': 0.16, ' was': 0.26, ' the': 0.02, ' shows': 0.02, ' obeys': 0.02, ' as': 0.02, ',': 0.06, ' has': 0.02, ' in': 0.02, ' that': 0.02, '?': 0.02}
NEW WORD  (
CURR PRE-NEW CONTEXT Why this this
NEW CONTEXT Why this this (
{'as': 0.06, 'the': 0.12, 'not': 0.02, 'and': 0.36, 'again': 0.02, 'it': 0.02, 'if': 0.02, 'one': 0.02, 'by': 0.02, 'for': 0.02, 'or': 0.1, 'which': 0.04, 'in': 0.02, 'un': 0.02, 'a': 0.06, 'but': 0.02, 'also': 0.02, 'especially': 0.02, 'rather': 0.02} {' when': 0.02, ',': 0.04, ' is': 0.12, ' (': 0.02, ' the': 0.2, '?': 0.14, ' caused': 0.02, '.': 0.04, ' in': 0.02, '': 0.02, ' has': 0.04, ' its': 0.06, ' fact': 0.04, ' :': 0.04, ' that': 0.02, ' so': 0.02, ' recent': 0.02, ' and': 0.04, ' was': 0.04, ' such': 0.02, ' also': 0.02}
NEW WORD  fact
CURR PRE-NEW CONTEXT Why this this
NEW CONTEXT Why this this fact
{':': 0.02, ' does': 0.02, ' will': 0.04, ' is': 0.44, ' should': 0.02, '.': 0.04, ' makes': 0.04, ',': 0.06, ' was': 0.04, ' can': 0.02, ' has': 0.08, ' really': 0.02, ' tells': 0.02, ' alone': 0.02, ' of': 0.02, ' remains': 0.02, ' that': 0.04, ' doesn': 0.02, 'oid': 0.02} {' was': 0.28, ' is': 0.34, ' "': 0.02, ',': 0.1, '?': 0.04, ' so': 0.06, ' the': 0.04, ' that': 0.02, ' causes': 0.02, ' and': 0.02, ' its': 0.02, ' makes': 0.04}
NEW WORD  :
CURR PRE-NEW CONTEXT Why this this
NEW CONTEXT Why this this :
{')': 0.06, ' the': 0.02, ' a': 0.02, ' 1': 0.02, ' I': 0.06, ' In': 0.04, ' A': 0.06, ' You': 0.08, ' It': 0.04, ' "': 0.14, ' The': 0.04, '\n': 0.18, ' you': 0.04, ' to': 0.02, ' -': 0.02, ' he': 0.02, ' If': 0.02, ' We': 0.02, '-': 0.04, ' To': 0.02, ' He': 0.02, ' This': 0.02} {'': 0.14, ' is': 0.16, ' was': 0.06, ' caused': 0.04, ' :': 0.36, ' the': 0.02, ' that': 0.02, ' because': 0.02, ' his': 0.02, ',': 0.02, ' since': 0.02, '?': 0.04, ' (': 0.02, ' and': 0.02, ' as': 0.02, ' it': 0.02}
{'?': 0.0, ' story': 0.1077067314256383, ' (': 0.0, ' fact': 0.07873626046931434, ' :': 0.0}
JS List [('?', 0.0), (' (', 0.0), (' :', 0.0), (' fact', 0.07873626046931434), (' story', 0.1077067314256383)]
highest JS word  story
CURR CONTEXT Why this this story JS 0.11582992188048757
NEW WORD ?
CURR PRE-NEW CONTEXT Why this this story
NEW CONTEXT Why this this story?
{' It': 0.1, ' Maybe': 0.02, ' In': 0.06, ' How': 0.04, ' "': 0.02, '\n': 0.38, ' Read': 0.02, ' The': 0.02, ' Here': 0.04, ' I': 0.06, ' Why': 0.04, ' Because': 0.02, ' There': 0.02, ' A': 0.02, ' As': 0.02, ' What': 0.02, ' Well': 0.04, ' Does': 0.02, ' Did': 0.04} {'Why this this was': 0.18, 'Why this this?': 0.06, 'Why this this is': 0.18, 'Why this this its': 0.02, 'Why this this has': 0.06, 'Why this this violates': 0.04, 'Why this this.': 0.04, 'Why this this made': 0.02, 'Why this this caused': 0.02, 'Why this this,': 0.08, 'Why this this so': 0.06, 'Why this this makes': 0.02, 'Why this this that': 0.02, 'Why this this the': 0.06, 'Why this this and': 0.02, 'Why this this in': 0.06, 'Why this this': 0.02, 'Why this this did': 0.02, 'Why this this a': 0.02}
NEW WORD  that
CURR PRE-NEW CONTEXT Why this this story
NEW CONTEXT Why this this story that
{' would': 0.02, ' I': 0.1, "'s": 0.06, ' is': 0.14, ' we': 0.12, ' the': 0.08, ' will': 0.02, ' no': 0.02, ' went': 0.02, ' was': 0.08, ' started': 0.02, ' can': 0.02, ' you': 0.08, ' has': 0.14, ' happened': 0.02, ' all': 0.02, ' they': 0.02, ' so': 0.02} {' a': 0.04, ' the': 0.1, ' not': 0.02, ' was': 0.22, '?': 0.04, ' so': 0.04, ' has': 0.06, ' tells': 0.02, ' made': 0.02, ' makes': 0.04, ' first': 0.04, ' is': 0.1, ' such': 0.04, ' its': 0.1, ' "': 0.02, ',': 0.04, '': 0.02, ' it': 0.02, ' cannot': 0.02}
NEW WORD  (
CURR PRE-NEW CONTEXT Why this this story
NEW CONTEXT Why this this story (
{'and': 0.46, 'the': 0.08, 'not': 0.02, 'as': 0.02, 'I': 0.02, 'we': 0.02, 'a': 0.04, 'or': 0.06, 'with': 0.02, 'which': 0.14, 'about': 0.02, 'that': 0.02, 'of': 0.02, 'it': 0.02, 'also': 0.02, 'in': 0.02} {' that': 0.04, ' for': 0.06, ' as': 0.06, ' the': 0.08, ' its': 0.04, ' also': 0.02, '?': 0.14, ' is': 0.06, ' 1988': 0.02, ' )': 0.1, ' =': 0.02, ' it': 0.04, ' story': 0.02, ' in': 0.02, ' which': 0.06, ',': 0.08, ' and': 0.06, ' history': 0.02, ' not': 0.02, ' "': 0.02, ' The': 0.02}
NEW WORD  was
CURR PRE-NEW CONTEXT Why this this story
NEW CONTEXT Why this this story was
{' told': 0.2, ' written': 0.22, ' even': 0.02, ' never': 0.02, ' part': 0.02, ' made': 0.06, ' meant': 0.04, ' developed': 0.02, ' a': 0.02, ' only': 0.02, ' about': 0.04, ' not': 0.08, ' originally': 0.04, ' all': 0.04, ' posted': 0.04, ' published': 0.06, ' supposed': 0.02, ' different': 0.02, ' brought': 0.02} {' so': 0.52, ' not': 0.14, ' is': 0.04, ' in': 0.02, ' the': 0.1, ',': 0.04, ' it': 0.02, ' :': 0.02, ' its': 0.04, ' such': 0.02, ' also': 0.02, ' made': 0.02}
NEW WORD  not
CURR PRE-NEW CONTEXT Why this this story
NEW CONTEXT Why this this story not
{' to': 0.12, ' worth': 0.02, ' only': 0.5, ' being': 0.12, ' about': 0.06, ' having': 0.02, ' a': 0.02, ' as': 0.02, ' yet': 0.02, ' just': 0.04, ' be': 0.02, ' known': 0.02, ' been': 0.02} {' has': 0.06, ' was': 0.1, ' it': 0.04, ' and': 0.04, ' is': 0.24, ' in': 0.06, ',': 0.14, ' its': 0.04, ' happens': 0.02, ' makes': 0.04, '?': 0.06, ' does': 0.04, ' being': 0.02, '': 0.02, ' made': 0.02, ' the': 0.02, ' remains': 0.02, ' itself': 0.02}
{'?': 0.0, ' that': 0.05782072394828691, ' (': 0.0, ' was': 0.060052730571041484, ' not': 0.0}
JS List [('?', 0.0), (' (', 0.0), (' not', 0.0), (' that', 0.05782072394828691), (' was', 0.060052730571041484)]
highest JS word  was
CURR CONTEXT Why this this story was JS 0.3900045654797509
2020-09-07 13:57:23.507786: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:57:23.508281: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 13:57:23.508295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-09-07 13:57:31.816699: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-09-07 13:57:31.866353: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-07 13:57:31.904532: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-09-07 13:57:31.904648: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: node082
2020-09-07 13:57:31.904672: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: node082
2020-09-07 13:57:31.904912: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.82.0
2020-09-07 13:57:31.905033: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.82.0
2020-09-07 13:57:31.905056: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.82.0
2020-09-07 13:57:31.905896: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-09-07 13:57:31.926323: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-09-07 13:57:31.927463: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e97d1c3670 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-07 13:57:31.927515: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
All model checkpoint weights were used when initializing TFGPT2LMHeadModel.

All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
All model checkpoint weights were used when initializing TFTransfoXLLMHeadModel.

All the weights of TFTransfoXLLMHeadModel were initialized from the model checkpoint at transfo-xl-wt103.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFTransfoXLLMHeadModel for predictions without further training.
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
NEW WORD  of
CURR PRE-NEW CONTEXT How
NEW CONTEXT How of
{' a': 0.18, ' it': 0.04, ' her': 0.02, ' the': 0.18, ' this': 0.12, ' all': 0.06, ' your': 0.12, ':': 0.02, ' these': 0.02, ' some': 0.02, ' that': 0.04, ' you': 0.06, ' us': 0.02, ' an': 0.04, ' his': 0.02, ' my': 0.02, ' those': 0.02} {' how': 0.6, ' about': 0.02, ' what': 0.16, ' How': 0.1, ' why': 0.04, ' Why': 0.02, ' What': 0.04, ' the': 0.02}
NEW WORD  at
CURR PRE-NEW CONTEXT How
NEW CONTEXT How at
{' this': 0.04, '?': 0.14, ' least': 0.08, '?"': 0.16, ' the': 0.1, ' all': 0.36, '-': 0.02, ' that': 0.04, '.': 0.02, '"': 0.02, ' first': 0.02} {' "': 0.1, ' how': 0.18, ',': 0.1, ' what': 0.14, ' at': 0.06, ' the': 0.1, ' where': 0.02, ' Is': 0.02, ' How': 0.04, ' Should': 0.04, ' Went': 0.02, ' Water': 0.02, ' should': 0.02, ' when': 0.02, ' :': 0.02, ' is': 0.02, ' for': 0.02, ' was': 0.02, ' about': 0.02, ' to': 0.02}
NEW WORD  it
CURR PRE-NEW CONTEXT How
NEW CONTEXT How it
{' works': 0.7, ' worked': 0.04, ' all': 0.04, ' does': 0.02, ' should': 0.02, ' looks': 0.02, ' makes': 0.02, ' Works': 0.02, ' is': 0.02, "'s": 0.02, ' will': 0.02, ' happens': 0.02, ' did': 0.02, ' was': 0.02} {' ended': 0.02, ' plays': 0.02, ' is': 0.16, ' was': 0.4, ' would': 0.04, ' behaved': 0.1, ' came': 0.04, ' Is': 0.02, ' performed': 0.02, ' turns': 0.02, ' "': 0.02, ' got': 0.04, ' has': 0.02, ' Went': 0.04, ' had': 0.02, ' can': 0.02}
NEW WORD :
CURR PRE-NEW CONTEXT How
NEW CONTEXT How:
{' I': 0.06, ' For': 0.02, '\n': 0.46, ' To': 0.12, ' Your': 0.02, ' You': 0.06, ' If': 0.04, ' Please': 0.02, ' to': 0.02, ' $': 0.02, ' The': 0.06, ' When': 0.02, ' 1': 0.02, ' What': 0.02, ' A': 0.02, ' See': 0.02} {'': 0.64, 'and': 0.08, 'the': 0.04, 'for': 0.02, 'with': 0.02, ',': 0.06, '@-@': 0.02, 'from': 0.02, 'in': 0.02, '.': 0.04, '(': 0.02, 'to': 0.02}
NEW WORD  (
CURR PRE-NEW CONTEXT How
NEW CONTEXT How (
{'or': 0.14, 'I': 0.04, 'if': 0.22, 'and': 0.16, 'Can': 0.06, 'If': 0.06, 'but': 0.02, 'at': 0.04, 'It': 0.02, 'So': 0.02, 's': 0.02, 'But': 0.02, 'How': 0.06, '?)': 0.02, 'should': 0.02, 'And': 0.04, 'you': 0.02, 'We': 0.02} {' is': 0.06, ' Thou': 0.02, '?': 0.16, '': 0.1, ' What': 0.1, ' Is': 0.14, ' and': 0.08, ' how': 0.1, ' Why': 0.04, ' How': 0.04, ' Feel': 0.02, ' Shall': 0.02, ' the': 0.02, ' Was': 0.02, ' Should': 0.02, ' Went': 0.02, "'s": 0.02, ' )': 0.02}
{' of': 0.0, ' at': 0.0, ' it': 0.024300595254825814, ':': 0.0, ' (': 0.0}
JS List [(' of', 0.0), (' at', 0.0), (':', 0.0), (' (', 0.0), (' it', 0.024300595254825814)]
highest JS word  it
CURR CONTEXT How it JS 0.12162836853262213
NEW WORD  has
CURR PRE-NEW CONTEXT How it
NEW CONTEXT How it has
{' changed': 0.08, ' affected': 0.06, ' all': 0.02, ' been': 0.24, ' happened': 0.08, ' to': 0.14, ' become': 0.02, ' occurred': 0.02, ' come': 0.06, ' turned': 0.04, ' worked': 0.12, ' done': 0.04, ' taken': 0.02, ' evolved': 0.02, ' gotten': 0.02, ' made': 0.02} {' been': 0.9, ' happened': 0.02, ' come': 0.02, ' appeared': 0.02, ' gained': 0.02, ' changed': 0.02}
NEW WORD  Is
CURR PRE-NEW CONTEXT How it
NEW CONTEXT How it Is
{' You': 0.02, ' The': 0.02, ' To': 0.08, ' Done': 0.12, ' Used': 0.06, ' And': 0.04, '\n': 0.1, ' A': 0.06, ' Made': 0.02, ' In': 0.08, ',': 0.04, ' Now': 0.04, ':': 0.1, ' to': 0.04, ' and': 0.06, ' Actually': 0.04, ' That': 0.02, '?': 0.02, ' Doing': 0.02, ' For': 0.02} {',': 0.14, ' @-@': 0.02, ' "': 0.06, ' Done': 0.06, ' is': 0.04, ' The': 0.04, ' a': 0.06, ' received': 0.02, ' to': 0.04, '': 0.02, ' what': 0.02, ' the': 0.04, ' It': 0.06, '?': 0.04, ' performed': 0.04, ' played': 0.04, ' made': 0.02, ' turned': 0.02, ' What': 0.02, ' changed': 0.02, '.': 0.04, ' Won': 0.04, ' Is': 0.02, ' :': 0.02, ' got': 0.02, ' Went': 0.02, ' was': 0.02}
NEW WORD  could
CURR PRE-NEW CONTEXT How it
NEW CONTEXT How it could
{' be': 0.34, ' affect': 0.06, ' hurt': 0.04, "'ve": 0.04, ' have': 0.22, ' work': 0.04, ' go': 0.04, ' happen': 0.12, ' end': 0.02, ' impact': 0.02, ' come': 0.02, ' make': 0.02, ' help': 0.02} {' move': 0.04, ' be': 0.48, ' function': 0.04, ' change': 0.04, ' work': 0.06, ' was': 0.02, ' become': 0.02, ' "': 0.02, ' turn': 0.04, ' have': 0.08, ' go': 0.04, ' is': 0.02, ' proceed': 0.02, ' come': 0.02, ' use': 0.02, ' approach': 0.02, ' the': 0.02}
NEW WORD  looks
CURR PRE-NEW CONTEXT How it
NEW CONTEXT How it looks
{':': 0.1, '\n': 0.16, ',': 0.04, ' in': 0.06, ' like': 0.36, ' from': 0.02, ' Like': 0.04, ' on': 0.04, ' to': 0.04, ' is': 0.02, ' at': 0.04, ' for': 0.02, ' and': 0.04, ' after': 0.02} {' and': 0.04, ' how': 0.06, ' was': 0.12, ' is': 0.28, ' when': 0.02, ',': 0.1, ' has': 0.04, ' turns': 0.02, ' looked': 0.04, ' plays': 0.02, ' its': 0.02, ' looks': 0.08, ' the': 0.02, '.': 0.02, ' fits': 0.02, ' â\x80\x94': 0.02, ' functions': 0.02, ' behaved': 0.02, "'s": 0.02, ' for': 0.02}
NEW WORD  happened
CURR PRE-NEW CONTEXT How it
NEW CONTEXT How it happened
{':': 0.58, ' to': 0.04, ' is': 0.08, ',': 0.08, ' Edit': 0.02, '.': 0.04, '?': 0.02, '\n': 0.12, ' and': 0.02} {' to': 0.14, '.': 0.18, ' happened': 0.02, ',': 0.3, ' when': 0.1, ' and': 0.06, ' during': 0.02, ' after': 0.02, ' was': 0.08, '': 0.02, ' "': 0.02, ' is': 0.02, ' :': 0.02}
{' has': 0.12088449749378077, ' Is': 0.07399656853925693, ' could': 0.03565300409756186, ' looks': 0.13353257093509113, ' happened': 0.08083087116889903}
JS List [(' could', 0.03565300409756186), (' Is', 0.07399656853925693), (' happened', 0.08083087116889903), (' has', 0.12088449749378077), (' looks', 0.13353257093509113)]
highest JS word  looks
CURR CONTEXT How it looks JS 0.1258409635761377
NEW WORD  at
CURR PRE-NEW CONTEXT How it looks
NEW CONTEXT How it looks at
{' it': 0.18, ' an': 0.04, ' these': 0.06, ' is': 0.08, ' the': 0.28, ' this': 0.12, ' you': 0.04, ' how': 0.02, ' our': 0.02, ' a': 0.04, '\n': 0.04, ' all': 0.02, ' them': 0.02, ' that': 0.02, ' your': 0.02} {' the': 0.4, ' plays': 0.02, ' how': 0.42, ' its': 0.06, ' what': 0.04, ' when': 0.02, ' looks': 0.02, ' itself': 0.02}
NEW WORD :
CURR PRE-NEW CONTEXT How it looks
NEW CONTEXT How it looks:
{' A': 0.04, '\n': 0.54, ' The': 0.1, ' This': 0.04, ' While': 0.02, ' If': 0.02, ' You': 0.04, ' All': 0.02, ' An': 0.04, ' It': 0.02, ' For': 0.02, '\n\n': 0.02, ' Well': 0.02, ' In': 0.02, ' At': 0.02, ' Like': 0.02} {'How it changed': 0.02, 'How it "': 0.04, 'How it was': 0.06, 'How it the': 0.14, 'How it @-@': 0.02, 'How it The': 0.02, 'How it': 0.12, 'How it and': 0.12, 'How it,': 0.26, 'How it it': 0.04, 'How it for': 0.02, 'How it :': 0.04, 'How it.': 0.02, 'How it?': 0.02, 'How it â\x80\x94': 0.02, 'How it to': 0.02, 'How it became': 0.02}
NEW WORD  was
CURR PRE-NEW CONTEXT How it looks
NEW CONTEXT How it looks was
{' the': 0.14, ' never': 0.04, ',': 0.04, ' a': 0.12, ' just': 0.12, ' this': 0.04, ' not': 0.04, ' all': 0.02, ' clear': 0.02, ' that': 0.08, ' she': 0.02, ' like': 0.02, ' simple': 0.02, ' one': 0.04, ' also': 0.02, ' originally': 0.02, ' when': 0.02, ' an': 0.04, ' pretty': 0.04, ' more': 0.02, ' revealed': 0.02, ' very': 0.02, ' there': 0.04} {' received': 0.02, ' changed': 0.1, ',': 0.1, ' used': 0.04, '.': 0.02, ' handled': 0.02, ' decided': 0.04, ' met': 0.02, ' released': 0.02, ' how': 0.02, ' was': 0.1, ' determined': 0.02, ' won': 0.06, ' made': 0.02, ' "': 0.04, ' turned': 0.08, ' brought': 0.04, ' said': 0.06, ' is': 0.02, ' opened': 0.04, ' moved': 0.02, ' to': 0.02, ' perceived': 0.02, ' affected': 0.02, ' understood': 0.02, ' the': 0.02}
NEW WORD  on
CURR PRE-NEW CONTEXT How it looks
NEW CONTEXT How it looks on
{' their': 0.02, ' television': 0.02, ' paper': 0.08, ' the': 0.48, ' our': 0.02, ' screen': 0.06, ' your': 0.04, ' his': 0.04, ' a': 0.06, ' this': 0.02, ' an': 0.02, ' Twitter': 0.02, ' video': 0.02, ' TV': 0.04, 'screen': 0.02, ' display': 0.02, ' top': 0.02} {' what': 0.02, ' how': 0.52, ' the': 0.2, ' its': 0.02, ' "': 0.02, ' on': 0.02, ' shows': 0.02, ',': 0.08, ' is': 0.04, ' in': 0.02, ' when': 0.02, ' looks': 0.02}
NEW WORD  its
CURR PRE-NEW CONTEXT How it looks
NEW CONTEXT How it looks its
{' a': 0.04, ' like': 0.32, " '": 0.04, '\n': 0.02, ' getting': 0.04, ' most': 0.08, ' been': 0.02, ' the': 0.1, ' not': 0.04, ' all': 0.02, ' way': 0.08, ' kind': 0.02, ' possible': 0.02, ' to': 0.02, ' name': 0.04, ' own': 0.02, ' more': 0.02, ' about': 0.02, ' done': 0.02, ' best': 0.02} {' way': 1.0}
{' at': 0.13275145970058078, ':': 0.0, ' was': 0.20146947190751813, ' on': 0.0, ' its': 0.0}
JS List [(':', 0.0), (' on', 0.0), (' its', 0.0), (' at', 0.13275145970058078), (' was', 0.20146947190751813)]
highest JS word  was
CURR CONTEXT How it looks was JS 0.18397906818083798
NEW WORD  it
CURR PRE-NEW CONTEXT How it looks was
NEW CONTEXT How it looks was it
{' would': 0.12, ' was': 0.34, "'s": 0.12, '?': 0.02, ' will': 0.06, ' took': 0.06, ' is': 0.04, ' got': 0.02, ' a': 0.02, ' on': 0.02, ' just': 0.02, ' can': 0.02, ' went': 0.02, ' might': 0.02, ' not': 0.04, ' didn': 0.02, ' had': 0.04} {' during': 0.04, ' when': 0.1, ' done': 0.04, '.': 0.08, ',': 0.12, ' to': 0.02, ' for': 0.02, ' seen': 0.02, ' used': 0.04, ' in': 0.06, ' was': 0.1, ' before': 0.02, ' decided': 0.02, ' and': 0.02, "'s": 0.02, ' released': 0.04, ' the': 0.06, ' by': 0.04, ' received': 0.02, ' perceived': 0.02, ' at': 0.02, ' :': 0.02, ' from': 0.02, ' built': 0.02, ' understood': 0.02}
NEW WORD  shown
CURR PRE-NEW CONTEXT How it looks was
NEW CONTEXT How it looks was shown
{' that': 0.16, ' in': 0.2, ' at': 0.06, ' on': 0.22, ' above': 0.02, ' by': 0.04, ' before': 0.04, ' to': 0.14, ',': 0.02, ' up': 0.02, ' off': 0.02, ' for': 0.02, ' as': 0.02, ' through': 0.02} {' shown': 0.32, ' in': 0.02, ' to': 0.44, ' :': 0.02, ' at': 0.02, '.': 0.02, ' was': 0.04, ' on': 0.06, ' the': 0.02, ' and': 0.02, ' during': 0.02}
NEW WORD  done
CURR PRE-NEW CONTEXT How it looks was
NEW CONTEXT How it looks was done
{' in': 0.18, ' to': 0.06, '.': 0.08, ' the': 0.02, ' with': 0.08, ' this': 0.02, ',': 0.02, ' very': 0.02, ' by': 0.16, ':': 0.08, ' using': 0.02, ' on': 0.08, ' when': 0.02, ' from': 0.02, ' after': 0.02, ' differently': 0.02, ' before': 0.02, ' pretty': 0.02, ' for': 0.02, ' well': 0.02, ' a': 0.02} {' to': 0.04, ' :': 0.02, '.': 0.2, ',': 0.18, ' with': 0.02, '': 0.02, ' in': 0.12, ' on': 0.06, ' during': 0.02, ' after': 0.04, ' by': 0.1, ' under': 0.02, ' a': 0.02, ' through': 0.04, ' how': 0.02, ' the': 0.04, ' when': 0.02, ' and': 0.02}
NEW WORD  looked
CURR PRE-NEW CONTEXT How it looks was
NEW CONTEXT How it looks was looked
{' for': 0.1, ' down': 0.1, ' at': 0.5, ' into': 0.04, ' up': 0.04, ' over': 0.1, ' upon': 0.02, ' like': 0.08, ' after': 0.02} {' into': 0.04, ' "': 0.02, ' for': 0.2, ' at': 0.52, ' in': 0.04, ' when': 0.02, ' upon': 0.02, '.': 0.02, ',': 0.04, ' towards': 0.02, ' after': 0.02, ' up': 0.02, ' by': 0.02}
NEW WORD  when
CURR PRE-NEW CONTEXT How it looks was
NEW CONTEXT How it looks was when
{' she': 0.06, ' I': 0.28, ' you': 0.12, ' they': 0.04, ' he': 0.1, ' the': 0.16, ' we': 0.12, ' it': 0.06, ' this': 0.02, ' my': 0.02, ' a': 0.02} {' the': 0.26, ',': 0.02, ' it': 0.56, ' in': 0.02, ' its': 0.02, ' he': 0.06, ' they': 0.02, ' @-@': 0.02, ' a': 0.02}
{' it': 0.006643686207925964, ' shown': 0.19392058220154873, ' done': 0.08921419447726195, ' looked': 0.011319272409841888, ' when': 0.1278230960737338}
JS List [(' it', 0.006643686207925964), (' looked', 0.011319272409841888), (' done', 0.08921419447726195), (' when', 0.1278230960737338), (' shown', 0.19392058220154873)]
highest JS word  shown
CURR CONTEXT How it looks was shown JS 0.33124076032066146
2020-09-07 14:10:51.453972: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 14:10:51.454075: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 14:10:51.454083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-09-07 14:10:57.104687: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-09-07 14:10:57.175429: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-07 14:10:57.180297: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-09-07 14:10:57.180359: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: node082
2020-09-07 14:10:57.180370: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: node082
2020-09-07 14:10:57.180506: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.82.0
2020-09-07 14:10:57.180557: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.82.0
2020-09-07 14:10:57.180567: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.82.0
2020-09-07 14:10:57.181060: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-09-07 14:10:57.193506: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-09-07 14:10:57.194404: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a46b8f6ae0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-07 14:10:57.194442: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
All model checkpoint weights were used when initializing TFGPT2LMHeadModel.

All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
All model checkpoint weights were used when initializing TFTransfoXLLMHeadModel.

All the weights of TFTransfoXLLMHeadModel were initialized from the model checkpoint at transfo-xl-wt103.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFTransfoXLLMHeadModel for predictions without further training.
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
NEW WORD 
CURR PRE-NEW CONTEXT The
NEW CONTEXT The
{'\n': 0.08, ' two': 0.06, ' people': 0.02, ' U': 0.02, ' on': 0.02, '.': 0.08, ' other': 0.02, ' a': 0.06, ' more': 0.04, ' for': 0.02, ' world': 0.02, ' of': 0.08, ' I': 0.02, ' same': 0.06, ',': 0.04, ' most': 0.02, "'s": 0.02, ' the': 0.06, ' ': 0.04, ' in': 0.02, ' and': 0.04, ' will': 0.02, ' "': 0.02, ' was': 0.02, ' to': 0.02, ' all': 0.02, '-': 0.04, ':': 0.02} {' I': 0.04, ' New': 0.06, ' Inquirer': 0.04, ' Two': 0.02, ' Washington': 0.06, ' John': 0.02, ' News': 0.02, ' Book': 0.06, ' Daily': 0.02, '': 0.08, ' Story': 0.12, ' Firefly': 0.04, ' Courier': 0.04, ' Mission': 0.02, ' ': 0.12, ' Son': 0.04, ' Little': 0.02, ' One': 0.02, ' Truth': 0.02, ' story': 0.02, ' Last': 0.02, ' Life': 0.02, ' Star': 0.02, ' Next': 0.02, ' Guardian': 0.02, ' Magazine': 0.02}
NEW WORD  in
CURR PRE-NEW CONTEXT The
NEW CONTEXT The in
{'ordinate': 0.04, 'organic': 0.02, 've': 0.04, ' the': 0.04, 'most': 0.02, '-': 0.36, 'efficiency': 0.02, 'patient': 0.02, 'built': 0.02, 'iqu': 0.02, 'born': 0.02, 'escap': 0.08, 'effect': 0.04, 'arg': 0.02, 'bred': 0.06, 'activity': 0.02, 'est': 0.02, 'offensive': 0.04, 'appropri': 0.02, 'ked': 0.02, 'aus': 0.02, 'alien': 0.02, 'version': 0.02} {' @-@': 0.06, '': 0.06, ' At': 0.02, ' In': 0.08, ' in': 0.38, ' the': 0.1, ' The': 0.02, ' at': 0.06, ' of': 0.12, ',': 0.02, ' Out': 0.02, "'s": 0.04, ' A': 0.02}
NEW WORD  last
CURR PRE-NEW CONTEXT The
NEW CONTEXT The last
{' time': 0.24, ' part': 0.02, ' thing': 0.2, ' of': 0.06, ' two': 0.1, ' is': 0.04, ' piece': 0.04, ' few': 0.06, ' major': 0.02, ' I': 0.02, ' we': 0.02, ' person': 0.04, ' year': 0.02, ' six': 0.02, ' day': 0.02, ' step': 0.02, ' five': 0.02, ' three': 0.02, ',': 0.02} {' "': 0.04, ' Post': 0.02, ' has': 0.02, ' of': 0.68, '.': 0.02, ' the': 0.02, ' had': 0.04, ' in': 0.02, ' mentioned': 0.02, ' is': 0.02, ' member': 0.02, ',': 0.04, ' a': 0.02, ' thing': 0.02}
NEW WORD  Post
CURR PRE-NEW CONTEXT The
NEW CONTEXT The Post
{' also': 0.04, "'s": 0.46, ':': 0.02, ' reports': 0.06, ' has': 0.1, ' broke': 0.02, ' reported': 0.12, ' and': 0.04, ' was': 0.02, ' is': 0.04, '-': 0.04, ' found': 0.02, ' will': 0.02} {' Post': 0.3, '': 0.08, ' and': 0.02, ' Star': 0.02, ' Tribune': 0.02, ' The': 0.12, ' the': 0.08, ' Times': 0.04, ' It': 0.02, ',': 0.08, ' (': 0.04, ' was': 0.02, ' is': 0.02, ' Review': 0.02, ' After': 0.02, ' Chronicle': 0.04, ' of': 0.02, ' on': 0.02, ' @-@': 0.02}
NEW WORD  most
CURR PRE-NEW CONTEXT The
NEW CONTEXT The most
{' difficult': 0.02, ' common': 0.24, ' important': 0.16, ' significant': 0.06, ' recent': 0.08, ' interesting': 0.08, ' frequent': 0.02, ' powerful': 0.02, ' likely': 0.04, ' obvious': 0.04, ' exciting': 0.04, ' popular': 0.08, ' famous': 0.04, ' surprising': 0.04, ' visible': 0.02, ' pressing': 0.02} {' "': 0.06, ' important': 0.12, ' recent': 0.06, ' prestigious': 0.04, ' widely': 0.02, ' the': 0.16, ' economically': 0.02, ' promising': 0.04, ' influential': 0.1, ' famous': 0.04, ' notable': 0.02, ' inventive': 0.02, ' in': 0.06, ' powerful': 0.02, ' prominent': 0.02, ' visible': 0.02, ' used': 0.02, ' @-@': 0.06, ' likely': 0.04, ' dramatic': 0.02, ' was': 0.02, ' of': 0.02}
{'': 0.004212595043515656, ' in': 0.0, ' last': 0.3139320464087919, ' Post': 0.011456875501600523, ' most': 0.002394843845306069}
JS List [(' in', 0.0), (' most', 0.002394843845306069), ('', 0.004212595043515656), (' Post', 0.011456875501600523), (' last', 0.3139320464087919)]
highest JS word  last
CURR CONTEXT The last JS 0.0
NEW WORD  was
CURR PRE-NEW CONTEXT The last
NEW CONTEXT The last was
{' taken': 0.02, ' my': 0.04, ' a': 0.26, ' about': 0.02, ' her': 0.04, ' the': 0.12, ' that': 0.04, ' made': 0.02, ' also': 0.04, ' by': 0.04, ',': 0.06, ' reported': 0.02, ' at': 0.04, ' not': 0.04, ' in': 0.02, ' more': 0.02, ' when': 0.02, ' on': 0.02, ' an': 0.06, ' to': 0.02, ' done': 0.04} {' in': 0.14, ' released': 0.04, ' an': 0.08, ' given': 0.02, ' on': 0.06, ' still': 0.02, ' performed': 0.02, ' also': 0.02, ' brought': 0.02, ' at': 0.08, ' produced': 0.02, ' the': 0.1, ' taken': 0.02, ' a': 0.14, ' shown': 0.02, ' not': 0.06, ' "': 0.02, ' to': 0.04, ' held': 0.02, ' as': 0.02, ' probably': 0.02, ',': 0.02}
NEW WORD  year
CURR PRE-NEW CONTEXT The last
NEW CONTEXT The last year
{' has': 0.2, ',': 0.16, ' or': 0.1, ' we': 0.06, ' of': 0.14, ' saw': 0.02, ' I': 0.02, ' will': 0.04, ' it': 0.02, ' and': 0.02, "'s": 0.06, ' the': 0.04, ' is': 0.02, ' was': 0.06, ' in': 0.02, ' with': 0.02} {' that': 0.04, ' the': 0.06, ' of': 0.7, ' in': 0.04, ' is': 0.04, ' from': 0.02, ' after': 0.04, ' for': 0.02, ',': 0.04}
NEW WORD  in
CURR PRE-NEW CONTEXT The last
NEW CONTEXT The last in
{' the': 0.3, ' my': 0.02, ' a': 0.3, ' line': 0.02, ' this': 0.06, ' our': 0.08, ' an': 0.08, ' their': 0.02, ' order': 0.02, ' her': 0.02, ' his': 0.06, ' that': 0.02} {' a': 0.42, ' the': 0.32, ' A': 0.02, ' an': 0.06, ' eight': 0.02, ' The': 0.02, ' series': 0.02, ' sequence': 0.02, ' every': 0.02, ' New': 0.02, ' three': 0.02, ' that': 0.02, ' line': 0.02}
NEW WORD  couple
CURR PRE-NEW CONTEXT The last
NEW CONTEXT The last couple
{' of': 0.78, ' years': 0.12, ' times': 0.04, ' hundred': 0.02, ' weeks': 0.04} {' of': 0.54, ' who': 0.06, ' they': 0.04, ' at': 0.02, ',': 0.04, ' had': 0.02, "'s": 0.08, ' a': 0.02, ' were': 0.08, ' to': 0.02, ' each': 0.04, ' was': 0.04}
NEW WORD  two
CURR PRE-NEW CONTEXT The last
NEW CONTEXT The last two
{' years': 0.32, ' months': 0.08, ' are': 0.04, ' letters': 0.02, ' games': 0.06, ' days': 0.04, ' were': 0.06, ' points': 0.02, ' decades': 0.04, ' to': 0.02, ' minutes': 0.02, ' albums': 0.02, ' episodes': 0.04, ' chapters': 0.02, ' examples': 0.02, ' weeks': 0.1, ' times': 0.02, ' words': 0.02, ' hours': 0.02, ' seasons': 0.02} {' of': 0.76, ' four': 0.04, ' sides': 0.04, ' from': 0.02, ',': 0.02, ' were': 0.02, ' and': 0.02, ' (': 0.02, ' two': 0.02, ' episodes': 0.02, ' had': 0.02}
{' was': 0.07335143185517645, ' year': 0.1295353812233765, ' in': 0.004623229471689863, ' couple': 0.0, ' two': 0.0050593899289876}
JS List [(' couple', 0.0), (' in', 0.004623229471689863), (' two', 0.0050593899289876), (' was', 0.07335143185517645), (' year', 0.1295353812233765)]
highest JS word  year
CURR CONTEXT The last year JS 0.08798152697488512
NEW WORD  was
CURR PRE-NEW CONTEXT The last year
NEW CONTEXT The last year was
{' when': 0.06, ' no': 0.04, ' a': 0.32, ' the': 0.28, ' just': 0.04, ' an': 0.02, ' also': 0.02, ' his': 0.02, ' great': 0.02, ' my': 0.02, ' filled': 0.02, ' very': 0.02, ' particularly': 0.04, ' one': 0.02, ' rough': 0.02, ' spent': 0.02, ' not': 0.02} {' 1917': 0.02, ' 1972': 0.04, ' 1976': 0.02, ' 1951': 0.04, ' in': 0.1, ' 1970': 0.04, ' 2006': 0.04, ' 1967': 0.06, ' 1997': 0.02, ' 1939': 0.02, ' 1982': 0.02, ' 1953': 0.04, ' 1914': 0.02, ' 1969': 0.06, ' 1998': 0.02, ' 1973': 0.06, ' 1919': 0.04, ' the': 0.04, ' 2015': 0.02, ' 1968': 0.04, ' 2002': 0.02, ' 1952': 0.04, ' 1958': 0.02, ' 1954': 0.02, ' 1963': 0.02, ' 1918': 0.04, ' 1936': 0.04, ' 2005': 0.02, ' 1975': 0.02}
NEW WORD  they
CURR PRE-NEW CONTEXT The last year
NEW CONTEXT The last year they
{"'ve": 0.36, ' have': 0.12, ' had': 0.14, ' are': 0.04, ' tried': 0.04, ' played': 0.02, "'re": 0.04, ' spent': 0.04, ' lost': 0.02, ' released': 0.02, ' could': 0.02, ' got': 0.02, ' were': 0.04, ' did': 0.02, ' gave': 0.02, ' also': 0.02, "'d": 0.02} {' together': 0.08, ' were': 0.34, ' played': 0.02, ' before': 0.02, ' lived': 0.06, ' was': 0.04, ' have': 0.02, '.': 0.02, ' at': 0.04, ' in': 0.1, ' had': 0.08, ',': 0.06, ' took': 0.02, ' worked': 0.02, ' met': 0.02, ' spent': 0.02, ' are': 0.02, ' appeared': 0.02}
NEW WORD  we
CURR PRE-NEW CONTEXT The last year
NEW CONTEXT The last year we
{' had': 0.18, ' got': 0.06, ' have': 0.18, "'ve": 0.36, ' will': 0.02, ' did': 0.06, ' made': 0.02, ' saw': 0.04, ' tried': 0.02, ' spent': 0.02, ' are': 0.02, ' haven': 0.02} {',': 0.2, ' know': 0.02, ' are': 0.12, ' of': 0.02, '.': 0.02, ' when': 0.08, ' a': 0.06, ' about': 0.02, ' before': 0.06, ' were': 0.04, ' show': 0.02, ' the': 0.06, ' us': 0.04, ' see': 0.02, ' all': 0.08, ' our': 0.1, ' in': 0.04}
NEW WORD  in
CURR PRE-NEW CONTEXT The last year
NEW CONTEXT The last year in
{' China': 0.02, ' San': 0.02, ' New': 0.04, ' the': 0.18, ' terms': 0.02, ' Washington': 0.04, ' particular': 0.06, ' Paris': 0.02, ' all': 0.02, ' a': 0.28, ' my': 0.02, ' which': 0.08, ' our': 0.06, ' Berlin': 0.02, ' Afghanistan': 0.02, ' one': 0.02, ' office': 0.02, ' fact': 0.02, ' Texas': 0.02, ' some': 0.02} {' 1975': 0.06, ' 2004': 0.02, ' the': 0.22, ' which': 0.52, ' his': 0.04, ' 1919': 0.02, ' 1970': 0.02, ' a': 0.06, ' 2000': 0.02, ' place': 0.02}
NEW WORD  or
CURR PRE-NEW CONTEXT The last year
NEW CONTEXT The last year or
{' so': 0.92, ' two': 0.08} {' about': 0.02, ' so': 0.3, ' of': 0.02, ' early': 0.04, ' in': 0.1, ' the': 0.06, ' or': 0.06, ' two': 0.02, ' and': 0.08, ' a': 0.04, ',': 0.02, ' five': 0.02, ' that': 0.02, ' another': 0.02, ' year': 0.02, ' when': 0.02, '.': 0.04, ' was': 0.02, ' for': 0.02, ' more': 0.02, ' three': 0.02, ' last': 0.02}
{' was': 0.0, ' they': 0.2089232766938681, ' we': 0.0, ' in': 0.17964329708039167, ' or': 0.000579858065573465}
JS List [(' was', 0.0), (' we', 0.0), (' or', 0.000579858065573465), (' in', 0.17964329708039167), (' they', 0.2089232766938681)]
highest JS word  they
CURR CONTEXT The last year they JS 0.2628595181348937
NEW WORD  together
CURR PRE-NEW CONTEXT The last year they
NEW CONTEXT The last year they together
{',': 0.04, ' got': 0.06, ' played': 0.06, ' were': 0.1, ' produced': 0.02, ' and': 0.02, ' had': 0.14, ' have': 0.14, ' brought': 0.02, ' spent': 0.08, ' ran': 0.02, ' lost': 0.02, ' did': 0.02, ' gave': 0.04, ' put': 0.02, ' made': 0.02, ' worked': 0.02, ' held': 0.02, ' won': 0.02, ' managed': 0.04, ' raised': 0.02, ' started': 0.04, ' would': 0.02} {' together': 0.02, ' they': 0.02, '.': 0.2, ' was': 0.16, ',': 0.42, ' (': 0.02, ' in': 0.04, ' the': 0.02, ' is': 0.04, ' and': 0.04, ' ;': 0.02}
NEW WORD  lost
CURR PRE-NEW CONTEXT The last year they
NEW CONTEXT The last year they lost
{' in': 0.06, ' by': 0.02, ' 15': 0.06, ' to': 0.14, ' a': 0.06, ' 8': 0.02, ' their': 0.08, ' 5': 0.06, ' three': 0.02, ' the': 0.06, ' seven': 0.02, ',': 0.04, ' 6': 0.02, '.': 0.02, ' $': 0.02, ' more': 0.06, ' 3': 0.02, ' nearly': 0.04, ' five': 0.02, ' almost': 0.06, ' two': 0.02, ' four': 0.04, ' all': 0.02, ' every': 0.02} {' their': 0.2, ' as': 0.06, ' the': 0.38, ',': 0.08, ' and': 0.06, ' that': 0.02, ' a': 0.02, ' there': 0.02, ' place': 0.02, '.': 0.06, ' they': 0.04, ' ;': 0.02, ' when': 0.02}
NEW WORD  in
CURR PRE-NEW CONTEXT The last year they
NEW CONTEXT The last year they in
{'ve': 0.1, ' the': 0.24, ' fact': 0.2, ' a': 0.02, 'ked': 0.2, ' my': 0.02, ' their': 0.04, ' this': 0.04, 'vent': 0.02, ' every': 0.02, ' most': 0.02, ' turn': 0.02, ' some': 0.04, ' any': 0.02} {' their': 0.2, ' the': 0.56, ' town': 0.02, ' a': 0.04, ' school': 0.02, ' existence': 0.04, ' The': 0.06, ' place': 0.02, ',': 0.02, ' this': 0.02}
NEW WORD  released
CURR PRE-NEW CONTEXT The last year they
NEW CONTEXT The last year they released
{' an': 0.08, ' the': 0.04, ' one': 0.06, ' a': 0.44, ' more': 0.04, ' two': 0.04, ' three': 0.04, ' their': 0.14, ' another': 0.02, ' 1': 0.02, ' "': 0.02, ' several': 0.02, ' 2': 0.02, ' over': 0.02} {' the': 0.28, '.': 0.12, ',': 0.14, ' together': 0.06, ' two': 0.04, ' their': 0.04, ' them': 0.02, ' a': 0.08, ' The': 0.02, ' release': 0.02, ' :': 0.02, ' on': 0.02, ' in': 0.02, ' and': 0.02, ' with': 0.02, ' it': 0.02, ' as': 0.02, ' before': 0.02, ' an': 0.02}
NEW WORD  attended
CURR PRE-NEW CONTEXT The last year they
NEW CONTEXT The last year they attended
{' college': 0.02, ' the': 0.4, ' a': 0.22, ',': 0.04, ' in': 0.02, ' about': 0.02, ' an': 0.04, ' events': 0.02, ' only': 0.02, ' to': 0.02, ' one': 0.04, ' some': 0.02, ' three': 0.04, ' this': 0.02, ' more': 0.02, ' at': 0.02, ' all': 0.02} {' ;': 0.04, ' church': 0.02, ',': 0.2, ' together': 0.04, ' classes': 0.02, '.': 0.14, ' was': 0.06, ' a': 0.06, ' school': 0.06, ' attend': 0.02, ' so': 0.04, ' meetings': 0.06, ' at': 0.02, ' the': 0.06, ' there': 0.02, ' attended': 0.04, ' attending': 0.02, ' before': 0.02, ' is': 0.02, ' The': 0.02, ' attendance': 0.02}
{' together': 0.048171945417331476, ' lost': 0.07626835600888376, ' in': 0.028244880117967105, ' released': 0.21565786959562544, ' attended': 0.19764620436166114}
JS List [(' in', 0.028244880117967105), (' together', 0.048171945417331476), (' lost', 0.07626835600888376), (' attended', 0.19764620436166114), (' released', 0.21565786959562544)]
highest JS word  released
CURR CONTEXT The last year they released JS 0.44216177385425515
2020-09-07 14:23:51.542340: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 14:23:51.542473: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 14:23:51.542485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-09-07 14:23:58.101587: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-09-07 14:23:58.167226: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-07 14:23:58.172299: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-09-07 14:23:58.172365: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: node082
2020-09-07 14:23:58.172376: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: node082
2020-09-07 14:23:58.172529: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.82.0
2020-09-07 14:23:58.172583: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.82.0
2020-09-07 14:23:58.172593: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.82.0
2020-09-07 14:23:58.173083: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-09-07 14:23:58.184781: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-09-07 14:23:58.185679: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5575df7b3c30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-07 14:23:58.185718: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
All model checkpoint weights were used when initializing TFGPT2LMHeadModel.

All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
All model checkpoint weights were used when initializing TFTransfoXLLMHeadModel.

All the weights of TFTransfoXLLMHeadModel were initialized from the model checkpoint at transfo-xl-wt103.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFTransfoXLLMHeadModel for predictions without further training.
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
NEW WORD  is
CURR PRE-NEW CONTEXT My
NEW CONTEXT My is
{' the': 0.14, ' in': 0.04, ' very': 0.02, ' an': 0.14, ' not': 0.08, ' pretty': 0.06, ' really': 0.04, ' my': 0.06, ' going': 0.02, ' just': 0.04, ' a': 0.16, ' all': 0.02, ' that': 0.02, ' so': 0.04, ' here': 0.02, ' looking': 0.04, ' more': 0.02, ' about': 0.02, ' also': 0.02} {',': 0.04, ' of': 0.04, ' My': 0.2, ' am': 0.02, ' a': 0.16, ' the': 0.08, ' and': 0.02, ' as': 0.02, '?': 0.06, ' my': 0.12, ' you': 0.02, ' I': 0.04, ' there': 0.02, '.': 0.02, ' an': 0.04, ' So': 0.02, '!': 0.02, ' not': 0.02, '...': 0.02, ' in': 0.02}
NEW WORD  to
CURR PRE-NEW CONTEXT My
NEW CONTEXT My to
{' the': 0.04, ' be': 0.2, ' think': 0.06, ' you': 0.06, ' love': 0.02, ' do': 0.06, ' keep': 0.02, '-': 0.02, ' work': 0.06, ' go': 0.02, ' move': 0.02, ' get': 0.08, ' my': 0.04, ' read': 0.02, ' this': 0.04, ' make': 0.04, ' learn': 0.02, 'on': 0.02, ' say': 0.08, ' understand': 0.02, ' give': 0.02, ' have': 0.04} {' My': 0.68, ' to': 0.02, ' be': 0.02, ' my': 0.06, ' the': 0.06, '...': 0.06, ' I': 0.02, ' a': 0.04, ' You': 0.02, ' The': 0.02}
NEW WORD  family
CURR PRE-NEW CONTEXT My
NEW CONTEXT My family
{' have': 0.04, ' and': 0.1, ' has': 0.08, ' was': 0.2, ' is': 0.16, ',': 0.12, ' never': 0.02, ' also': 0.02, "'s": 0.1, '.': 0.02, ' will': 0.02, ' are': 0.02, ' came': 0.02, ' had': 0.02, ' can': 0.02, ' knows': 0.02, ' would': 0.02} {"'s": 0.06, '.': 0.12, '...': 0.04, ' of': 0.06, ' my': 0.1, ',': 0.1, ' with': 0.08, ' the': 0.08, ' business': 0.02, ' My': 0.04, ' @-@': 0.04, ' â\x80\x94': 0.02, ' and': 0.12, ' a': 0.04, ' I': 0.02, ' made': 0.02, '?': 0.02, ' when': 0.02}
NEW WORD  was
CURR PRE-NEW CONTEXT My
NEW CONTEXT My was
{' about': 0.02, ' trying': 0.04, ' lucky': 0.02, ' just': 0.1, ' surprised': 0.04, ' sitting': 0.02, ' thinking': 0.04, ',': 0.06, ' going': 0.06, ' an': 0.04, ' so': 0.04, ' really': 0.06, ' doing': 0.02, ' told': 0.02, ' able': 0.04, ' never': 0.02, ' the': 0.08, ' my': 0.04, ' hoping': 0.02, ' there': 0.02, ' not': 0.08, ' a': 0.02, ' born': 0.02, ' in': 0.04, ' very': 0.02, ' looking': 0.02} {' in': 0.06, ' a': 0.16, ' my': 0.1, ' "': 0.04, ' the': 0.08, ' :': 0.02, ' My': 0.26, ',': 0.08, '.': 0.04, '...': 0.02, ' of': 0.04, "'s": 0.02, ' there': 0.02, '?': 0.02, ' as': 0.02, ' very': 0.02}
NEW WORD  one
CURR PRE-NEW CONTEXT My
NEW CONTEXT My one
{' goal': 0.08, ' complaint': 0.04, ' regret': 0.08, ' big': 0.18, ' weakness': 0.02, ' major': 0.08, ' issue': 0.06, ' great': 0.08, ' problem': 0.04, ' hope': 0.02, ' and': 0.06, ' thing': 0.06, '-': 0.08, ' constant': 0.02, ' wish': 0.02, ' concern': 0.02, ' of': 0.02, ' catch': 0.02, ' fear': 0.02} {' I': 0.64, ',': 0.02, ' My': 0.16, ' to': 0.02, ' the': 0.04, ' @-@': 0.02, '...': 0.04, "'s": 0.02, ' my': 0.02, ' was': 0.02}
{' is': 0.05360775191134682, ' to': 0.1787151512636385, ' family': 0.060652572277922534, ' was': 0.05978521940077613, ' one': 0.0}
JS List [(' one', 0.0), (' is', 0.05360775191134682), (' was', 0.05978521940077613), (' family', 0.060652572277922534), (' to', 0.1787151512636385)]
highest JS word  to
CURR CONTEXT My to JS 0.07462108639183879
NEW WORD  do
CURR PRE-NEW CONTEXT My to
NEW CONTEXT My to do
{' was': 0.02, ' this': 0.16, ' with': 0.18, ' something': 0.06, ' some': 0.04, ' the': 0.04, ' is': 0.1, '."': 0.02, ' my': 0.02, ' it': 0.06, ' things': 0.02, ' that': 0.04, ' well': 0.02, '?': 0.02, ',': 0.04, '"': 0.02, ',"': 0.04, ' one': 0.02, '.': 0.04, ' and': 0.02, ' a': 0.02} {' my': 0.08, ',': 0.08, ' My': 0.18, ' I': 0.06, ' It': 0.02, ' and': 0.08, ' Do': 0.04, ' to': 0.06, '': 0.04, ' (': 0.02, '...': 0.02, ' You': 0.02, '?': 0.02, ' of': 0.02, ' so': 0.02, ' a': 0.02, ' In': 0.04, ' as': 0.02, ' "': 0.06, ' =': 0.02, ' Your': 0.02, ' was': 0.02, '.': 0.02, ' To': 0.02}
NEW WORD  My
CURR PRE-NEW CONTEXT My to
NEW CONTEXT My to My
{' Grave': 0.02, ' B': 0.06, 'self': 0.42, ' Mom': 0.04, ' P': 0.02, ' Face': 0.02, ' House': 0.04, ' Love': 0.06, ' T': 0.04, ' Room': 0.02, ' Mind': 0.04, '\n': 0.02, ' F': 0.02, ' Friends': 0.02, ' Life': 0.04, ' Home': 0.02, ' Dear': 0.02, ' S': 0.02, ' School': 0.02, ' Heart': 0.02, ' Own': 0.02} {',': 0.06, ' My': 0.7, ' Your': 0.02, ' the': 0.02, '...': 0.06, '': 0.02, ' "': 0.02, ' Heart': 0.04, ' Dear': 0.02, ' I': 0.02, ' Life': 0.02}
NEW WORD  say
CURR PRE-NEW CONTEXT My to
NEW CONTEXT My to say
{',': 0.32, ' that': 0.22, ' to': 0.02, ' this': 0.08, ' you': 0.02, ' thank': 0.04, ' I': 0.1, ' the': 0.06, ' they': 0.04, ' he': 0.02, ' a': 0.02, ' it': 0.04, ' there': 0.02} {' with': 0.02, ' "': 0.06, ' and': 0.06, ' I': 0.1, ' a': 0.04, ',': 0.2, ' or': 0.06, ' My': 0.04, ' at': 0.02, ' is': 0.06, ' said': 0.04, ' in': 0.04, '...': 0.04, ' â\x80\x94': 0.02, ' about': 0.04, ' meant': 0.02, ' to': 0.04, ' has': 0.02, ' was': 0.02, '?': 0.02, ' â\x80\x93': 0.02, ' the': 0.02}
NEW WORD  start
CURR PRE-NEW CONTEXT My to
NEW CONTEXT My to start
{' off': 0.28, ' with': 0.3, ' a': 0.04, ' by': 0.06, ' on': 0.02, ' writing': 0.02, ' this': 0.06, ' talking': 0.02, ' and': 0.02, ' from': 0.04, ' you': 0.02, ' making': 0.02, ' getting': 0.02, ' out': 0.02, ' thinking': 0.04, ' the': 0.02} {' or': 0.02, ' the': 0.14, ' off': 0.02, ' and': 0.04, '.': 0.08, ' My': 0.1, ' I': 0.04, ' start': 0.02, ' as': 0.04, ' with': 0.06, ' my': 0.06, ' from': 0.02, '...': 0.06, ' "': 0.06, ' a': 0.06, ' in': 0.02, ' out': 0.02, ' of': 0.02, ' The': 0.02, ' it': 0.02, ' to': 0.02, ' on': 0.02, ' started': 0.02, ' new': 0.02}
NEW WORD  "
CURR PRE-NEW CONTEXT My to
NEW CONTEXT My to "
{'save': 0.02, 'I': 0.06, 'b': 0.06, 'the': 0.04, 'go': 0.06, 'f': 0.02, 'C': 0.04, 'Go': 0.04, 'The': 0.08, 'F': 0.02, 'do': 0.04, 'd': 0.06, 'G': 0.02, 'B': 0.04, 'make': 0.04, 'H': 0.02, 'P': 0.02, 'R': 0.02, 'get': 0.02, 'read': 0.02, 'just': 0.02, 'c': 0.02, 'S': 0.02, 'p': 0.02, 't': 0.02, 's': 0.02, 'E': 0.02, 'play': 0.02, 'A': 0.02, 'L': 0.02, '\n': 0.02, 'D': 0.04} {' [': 0.06, ' give': 0.1, '...': 0.14, ' make': 0.04, ' tell': 0.02, ' "': 0.2, ' be': 0.04, '': 0.08, ' serve': 0.02, ' the': 0.1, ' go': 0.02, ' fight': 0.02, ' I': 0.02, ' have': 0.02, ' my': 0.02, ' My': 0.02, ' take': 0.02, ' keep': 0.02, ' win': 0.02, ' The': 0.02}
{' do': 0.06191910212693631, ' My': 0.04247475919884931, ' say': 0.025683653204823746, ' start': 0.2151270511499367, ' "': 0.0}
JS List [(' "', 0.0), (' say', 0.025683653204823746), (' My', 0.04247475919884931), (' do', 0.06191910212693631), (' start', 0.2151270511499367)]
highest JS word  start
CURR CONTEXT My to start JS 0.1805919384952762
NEW WORD 
CURR PRE-NEW CONTEXT My to start
NEW CONTEXT My to start
{' with': 0.5, ' you': 0.02, ' working': 0.02, ',': 0.02, ' off': 0.14, ' by': 0.06, ' on': 0.02, '.': 0.02, ' making': 0.02, ' a': 0.04, ' this': 0.02, ' using': 0.02, ' from': 0.02, ' now': 0.02, ' out': 0.04, ' the': 0.02} {' With': 0.02, ',': 0.12, ' or': 0.04, ' "': 0.02, ' to': 0.06, ' My': 0.18, ' at': 0.02, ' =': 0.02, ' anew': 0.06, ' The': 0.04, ' I': 0.04, ' an': 0.02, ' off': 0.02, '': 0.02, ' as': 0.06, ' with': 0.08, '.': 0.02, ' of': 0.02, ' a': 0.02, ' start': 0.02, ' was': 0.02, ' started': 0.02, '...': 0.02, ' on': 0.02, ' work': 0.02}
NEW WORD  My
CURR PRE-NEW CONTEXT My to start
NEW CONTEXT My to start My
{' to': 0.26, ' first': 0.04, ' account': 0.02, ' heart': 0.04, ' new': 0.06, ' last': 0.02, ' Love': 0.02, ' First': 0.04, ' own': 0.06, ' Little': 0.06, ',': 0.02, ' To': 0.06, ' day': 0.04, ' list': 0.02, ' business': 0.02, 'self': 0.04, ' name': 0.04, ' little': 0.04, ' Name': 0.02, ' Life': 0.02, ' work': 0.02, ' time': 0.02, ' life': 0.02} {' Life': 0.04, ' I': 0.28, ' as': 0.04, ' My': 0.08, ' Home': 0.02, ' To': 0.08, ' and': 0.04, ' the': 0.02, ' Story': 0.02, ' @-@': 0.02, ',': 0.04, ' "': 0.04, '.': 0.1, ' Work': 0.04, '...': 0.06, ' The': 0.02, ' You': 0.02, ' Day': 0.04}
NEW WORD  start
CURR PRE-NEW CONTEXT My to start
NEW CONTEXT My to start start
{' out': 0.02, ' with': 0.2, ' thinking': 0.02, ' the': 0.12, ' talking': 0.02, ' my': 0.08, ' by': 0.04, ' on': 0.04, ' this': 0.02, ' off': 0.04, ' again': 0.02, ' getting': 0.02, ' over': 0.08, ' to': 0.04, ' looking': 0.02, '.': 0.02, ' using': 0.04, ' and': 0.02, ' in': 0.02, ' here': 0.04, ' working': 0.02, ' writing': 0.02, ' up': 0.02, ' making': 0.02} {' My': 0.04, ' of': 0.06, ' as': 0.02, '.': 0.08, ' "': 0.04, ' The': 0.02, ' ;': 0.02, ' I': 0.06, ' in': 0.06, ' with': 0.04, ' the': 0.06, ' a': 0.02, ',': 0.1, ' is': 0.04, ' an': 0.02, ' was': 0.02, ' and': 0.06, ' came': 0.02, ' new': 0.04, ' at': 0.04, ' work': 0.06, ' off': 0.02, ' (': 0.02, ' anew': 0.02, ' my': 0.02}
NEW WORD  it
CURR PRE-NEW CONTEXT My to start
NEW CONTEXT My to start it
{' by': 0.04, ' off': 0.82, ' with': 0.02, ' was': 0.06, ' out': 0.02, ' all': 0.04} {' was': 0.16, ' it': 0.02, ',': 0.14, ' and': 0.04, ' with': 0.12, ' "': 0.04, ' up': 0.08, ' is': 0.1, ' had': 0.04, ' in': 0.04, ' off': 0.04, ' a': 0.02, ' on': 0.02, '.': 0.1, ' to': 0.04}
NEW WORD .
CURR PRE-NEW CONTEXT My to start
NEW CONTEXT My to start.
{'\n': 0.32, ' This': 0.02, ' As': 0.02, ' It': 0.14, ' But': 0.04, ' My': 0.08, ' When': 0.02, ' The': 0.04, ' I': 0.16, ' A': 0.02, ' How': 0.02, ' What': 0.02, '\n\n': 0.02, ' That': 0.02, ' All': 0.02, ' You': 0.02, ' We': 0.02} {'My to.': 0.04, 'My to :': 0.04, 'My to My': 0.18, 'My to I': 0.06, 'My to': 0.08, 'My to a': 0.06, 'My to (': 0.08, 'My to,': 0.1, 'My to "': 0.02, 'My to was': 0.06, 'My to Oh': 0.02, "My to's": 0.02, 'My to or': 0.02, 'My to the': 0.08, 'My to It': 0.02, 'My to and': 0.04, 'My to )': 0.02, 'My to â\x80\x93': 0.02, 'My to You': 0.02, 'My to...': 0.02}
{'': 0.172111052140203, ' My': 0.005059389928987683, ' start': 0.14088828148169796, ' it': 0.35693803709098515, '.': 0.0}
JS List [('.', 0.0), (' My', 0.005059389928987683), (' start', 0.14088828148169796), ('', 0.172111052140203), (' it', 0.35693803709098515)]
highest JS word  it
CURR CONTEXT My to start it JS 0.23784769694144262
NEW WORD  â
CURR PRE-NEW CONTEXT My to start it
NEW CONTEXT My to start it â
{' and': 0.02, '€': 0.04, '�': 0.22, '"': 0.2, '½': 0.1, ' �': 0.08, '!': 0.02, '.': 0.04, ',': 0.06, 't': 0.02, '\n': 0.04, ' "': 0.04, '¯': 0.04, '¨': 0.02, 'â': 0.02, ' I': 0.02, ',"': 0.02} {' for': 0.06, ' of': 0.04, ' "': 0.06, ' the': 0.18, ' an': 0.08, ' a': 0.12, ' all': 0.02, ' and': 0.04, ' â\x80\x94': 0.04, ' which': 0.12, ' with': 0.02, ' something': 0.02, ',': 0.02, ' at': 0.04, ' he': 0.02, ' in': 0.02, ' that': 0.04, ' his': 0.02, ' more': 0.02, ' or': 0.02}
NEW WORD  is
CURR PRE-NEW CONTEXT My to start it
NEW CONTEXT My to start it is
{' no': 0.04, ' that': 0.1, ' a': 0.16, ' with': 0.08, ' to': 0.06, ' the': 0.02, ' really': 0.02, ' very': 0.08, ' as': 0.02, ' about': 0.02, ' my': 0.02, ' just': 0.02, ' not': 0.04, ' because': 0.02, ' easy': 0.04, ' probably': 0.02, ' on': 0.02, ' good': 0.02, ' true': 0.04, ' I': 0.02, ' so': 0.02, ':': 0.02, ',': 0.04, ' clear': 0.02, ' from': 0.02, ' simple': 0.02} {' a': 0.24, ' to': 0.04, ' currently': 0.02, ' not': 0.04, ' that': 0.04, ' the': 0.1, ' an': 0.06, ' now': 0.1, ' and': 0.02, ' based': 0.02, '.': 0.06, ' of': 0.02, ' about': 0.02, ',': 0.04, ' from': 0.02, ' what': 0.02, ' as': 0.04, ' (': 0.02, ' also': 0.02, ' "': 0.04, ' possible': 0.02}
NEW WORD  all
CURR PRE-NEW CONTEXT My to start it
NEW CONTEXT My to start it all
{' off': 0.94, ' out': 0.04, ' right': 0.02} {' a': 0.04, '.': 0.1, ' out': 0.1, ' was': 0.06, ' off': 0.06, ' on': 0.06, ' to': 0.06, ' is': 0.04, ' with': 0.08, '?': 0.04, '!': 0.04, ' at': 0.02, ' right': 0.06, ' up': 0.02, ' as': 0.04, ' the': 0.04, ',': 0.06, '...': 0.02, ' together': 0.04, ' all': 0.02}
NEW WORD  "
CURR PRE-NEW CONTEXT My to start it
NEW CONTEXT My to start it "
{'c': 0.02, 'g': 0.04, 'S': 0.04, 'C': 0.04, 'b': 0.02, 't': 0.06, 'I': 0.18, 's': 0.04, 'M': 0.04, 'My': 0.02, 'for': 0.04, 'The': 0.06, 'just': 0.04, 'm': 0.02, 'G': 0.02, 'n': 0.02, 'on': 0.02, 'w': 0.02, 'T': 0.02, 'D': 0.02, 'A': 0.02, 'go': 0.02, 'H': 0.02, 'B': 0.02, 'off': 0.04, 'to': 0.02, 'No': 0.02, 'L': 0.02, 'the': 0.02, 'p': 0.02} {' and': 0.02, ' was': 0.18, ' has': 0.04, ' would': 0.04, ' in': 0.04, '.': 0.04, ' from': 0.02, '...': 0.02, ' I': 0.06, ' came': 0.04, ' is': 0.14, ',': 0.04, ' doesn': 0.08, ' â\x80\x93': 0.02, ' "': 0.04, ' The': 0.02, ' for': 0.04, ' will': 0.04, ' you': 0.02, ' isn': 0.02, ' a': 0.02, ' with': 0.02}
NEW WORD  was
CURR PRE-NEW CONTEXT My to start it
NEW CONTEXT My to start it was
{' pretty': 0.06, ' very': 0.06, ' at': 0.02, ' easy': 0.02, ' because': 0.04, ' a': 0.14, ' to': 0.04, ' going': 0.04, ' about': 0.08, ' with': 0.02, ' always': 0.04, ' all': 0.02, ' for': 0.02, ' probably': 0.04, ' not': 0.02, ' on': 0.04, ' the': 0.06, ' just': 0.06, ' so': 0.06, ' this': 0.02, ' an': 0.02, ' when': 0.02, ' my': 0.02, ' great': 0.02, ' like': 0.02} {' also': 0.02, ' a': 0.28, ',': 0.08, ' set': 0.02, ' under': 0.02, ' being': 0.02, ' of': 0.04, ' based': 0.02, ' on': 0.02, ' the': 0.12, ' in': 0.02, ' from': 0.02, ' an': 0.02, ' "': 0.08, ' about': 0.06, ' his': 0.02, ' to': 0.02, ' supposed': 0.02, ' replaced': 0.02, ' so': 0.02, ' actually': 0.02, ' very': 0.02, '.': 0.02}
{' â\x80\x94': 0.06615206235944923, ' is': 0.04876808369640912, ' all': 0.2639913081683366, ' "': 0.0, ' was': 0.057932748169951205}
JS List [(' "', 0.0), (' is', 0.04876808369640912), (' was', 0.057932748169951205), (' â\x80\x94', 0.06615206235944923), (' all', 0.2639913081683366)]
highest JS word  all
CURR CONTEXT My to start it all JS 0.42272780523705944
2020-09-07 14:37:13.861019: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 14:37:13.861126: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/openmind/cudnn/cudnn-8.0-v5.1/lib64:/cm/shared/openmind/cuda/8.0/lib64:/cm/shared/openmind/cuda/8.0/lib
2020-09-07 14:37:13.861135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-09-07 14:37:19.602100: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2020-09-07 14:37:19.662870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-09-07 14:37:19.668108: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-09-07 14:37:19.668174: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: node082
2020-09-07 14:37:19.668183: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: node082
2020-09-07 14:37:19.668331: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.82.0
2020-09-07 14:37:19.668381: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.82.0
2020-09-07 14:37:19.668390: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.82.0
2020-09-07 14:37:19.668840: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-09-07 14:37:19.679972: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz
2020-09-07 14:37:19.680825: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563912ca8000 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-07 14:37:19.680867: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
All model checkpoint weights were used when initializing TFGPT2LMHeadModel.

All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
All model checkpoint weights were used when initializing TFTransfoXLLMHeadModel.

All the weights of TFTransfoXLLMHeadModel were initialized from the model checkpoint at transfo-xl-wt103.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFTransfoXLLMHeadModel for predictions without further training.
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
You might want to consider setting `add_space_before_punct_symbol=True` as an argument to the `tokenizer.encode()` to avoid tokenizing words with punctuation symbols to the `<unk>` token
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence
Setting `pad_token_id` to 0 (first `eos_token_id`) to generate sequence
NEW WORD  will
CURR PRE-NEW CONTEXT I
NEW CONTEXT I will
{' give': 0.02, ' be': 0.24, ' put': 0.02, ' start': 0.02, ' never': 0.02, ' take': 0.04, ' write': 0.02, ' show': 0.04, ' try': 0.06, ' tell': 0.02, ' make': 0.04, ' have': 0.04, ' say': 0.02, ' not': 0.12, ' use': 0.02, ' leave': 0.08, ' need': 0.02, ' return': 0.02, ' do': 0.02, ' keep': 0.04, ' send': 0.02, ' also': 0.02, ' work': 0.02, ' get': 0.02} {' I': 0.22, ' My': 0.02, ' have': 0.06, ' tell': 0.02, '...': 0.02, ' my': 0.06, ' myself': 0.02, ' Will': 0.02, ' am': 0.06, ',': 0.1, ' give': 0.04, '.': 0.06, ' you': 0.04, ' be': 0.08, '!': 0.02, ' You': 0.02, ' serve': 0.02, ' of': 0.02, ' keep': 0.02, ' make': 0.02, ' a': 0.02, ' and': 0.02, ' see': 0.02}
NEW WORD 's
CURR PRE-NEW CONTEXT I
NEW CONTEXT I's
{' gonna': 0.04, ' talking': 0.06, ' done': 0.02, ' just': 0.02, ' a': 0.18, ' going': 0.06, ' looking': 0.08, ' my': 0.04, ' not': 0.08, ' doing': 0.02, ' got': 0.02, ' in': 0.04, ' been': 0.06, ' no': 0.02, ' an': 0.06, ' about': 0.02, ' the': 0.02, ' also': 0.02, ' like': 0.02, ' always': 0.02, ' getting': 0.04, ' trying': 0.04, ' really': 0.02} {'': 0.52, '(': 0.02, 'and': 0.1, '"': 0.06, '@-@': 0.02, '.': 0.14, ',': 0.04, 'where': 0.02, 'the': 0.02, 'of': 0.02, 'which': 0.02, ';': 0.02}
NEW WORD  on
CURR PRE-NEW CONTEXT I
NEW CONTEXT I on
{' Friday': 0.02, ' the': 0.36, ' his': 0.04, ' Sunday': 0.06, ' my': 0.12, ' top': 0.02, ' this': 0.1, ' her': 0.02, ' Twitter': 0.06, ' behalf': 0.02, ' a': 0.04, ' one': 0.02, ' Monday': 0.04, ' Reddit': 0.04, ' that': 0.02, ' our': 0.02} {' to': 0.02, ' the': 0.24, ' I': 0.1, ' You': 0.02, ' authority': 0.02, ' My': 0.04, ' a': 0.06, ' with': 0.02, ' on': 0.02, ' both': 0.02, ' in': 0.06, ',': 0.04, ' and': 0.04, '.': 0.04, ' had': 0.04, ' command': 0.02, ' duty': 0.02, ' my': 0.06, '...': 0.02, ' myself': 0.02, ' about': 0.02, ' at': 0.02, ' was': 0.02, ' @-@': 0.02}
NEW WORD  be
CURR PRE-NEW CONTEXT I
NEW CONTEXT I be
{' doing': 0.04, ' allowed': 0.02, ' sure': 0.06, ' damned': 0.06, ' honest': 0.06, ' the': 0.04, ' happy': 0.08, ' able': 0.1, ' all': 0.02, ' very': 0.02, ' with': 0.02, ' in': 0.04, ' a': 0.14, ' there': 0.02, ' at': 0.04, ' surprised': 0.04, ' an': 0.02, ' so': 0.02, 'le': 0.02, ' my': 0.02, ',': 0.02, ' back': 0.02, ' your': 0.04, ' afraid': 0.02, ' sorry': 0.02} {' myself': 0.18, ' so': 0.02, ' I': 0.44, ' a': 0.04, ' to': 0.02, ' very': 0.04, ' told': 0.02, ' :': 0.02, ' in': 0.06, ',': 0.08, ' have': 0.02, ' of': 0.02, ' "': 0.02, ' my': 0.02}
NEW WORD 

CURR PRE-NEW CONTEXT I
NEW CONTEXT I

{'\n': 1.0} {'I I': 0.8, 'I myself': 0.02, 'I am': 0.02, 'I,': 0.1, 'I was': 0.02, 'I and': 0.04}
{' will': 0.05535224898690766, "'s": 0.0, ' on': 0.01118087102868419, ' be': 0.10404881627608642, '\n': 0.0}
JS List [("'s", 0.0), ('\n', 0.0), (' on', 0.01118087102868419), (' will', 0.05535224898690766), (' be', 0.10404881627608642)]
highest JS word  be
CURR CONTEXT I be JS 0.00771594541165976
NEW WORD  an
CURR PRE-NEW CONTEXT I be
NEW CONTEXT I be an
{' animal': 0.04, ' architect': 0.02, ' avid': 0.04, ' honest': 0.04, ' old': 0.02, ' expert': 0.02, ' asshole': 0.08, ' open': 0.02, ' ass': 0.02, ' absolute': 0.02, ' actor': 0.04, ' atheist': 0.06, ' excellent': 0.02, ' agent': 0.04, ' entrepreneur': 0.04, ' American': 0.04, ' idiot': 0.08, ' astronaut': 0.02, ' adult': 0.02, ' active': 0.06, ' advocate': 0.04, ' ambassador': 0.02, ' engineer': 0.04, ' important': 0.02, ' anarchist': 0.06, ' artist': 0.02, ' actress': 0.02, ' activist': 0.04} {' advocate': 0.12, ' A': 0.06, ' I': 0.32, ' experienced': 0.02, ' officer': 0.06, ' important': 0.02, ' enemy': 0.04, ' American': 0.02, ' ambassador': 0.02, ' employee': 0.06, ' observer': 0.04, ' operator': 0.04, ' Examiner': 0.04, ' "': 0.02, ' admirer': 0.02, ' Officer': 0.02, ' a': 0.02, ' instructor': 0.02, ',': 0.02, ' artist': 0.02}
NEW WORD  "
CURR PRE-NEW CONTEXT I be
NEW CONTEXT I be "
{'out': 0.02, 'no': 0.06, 'the': 0.14, 'I': 0.04, 'g': 0.04, 's': 0.04, 'un': 0.02, 't': 0.02, 'so': 0.02, 'The': 0.08, 'B': 0.02, 'in': 0.04, 'happy': 0.02, 'good': 0.02, 'f': 0.04, 'c': 0.08, 'h': 0.02, 'a': 0.06, 'very': 0.04, 'd': 0.02, 'S': 0.02, 'P': 0.02, 'p': 0.04, 'really': 0.02, 'too': 0.02, 'one': 0.02, 'st': 0.02} {' most': 0.04, ' "': 0.12, ' more': 0.04, ' a': 0.32, ' my': 0.04, ' to': 0.02, ' I': 0.02, ' the': 0.04, ' of': 0.04, ' deeply': 0.02, '': 0.02, ' very': 0.08, ' and': 0.02, ' in': 0.02, ' one': 0.04, ' pretty': 0.02, ' proud': 0.02, ' crazy': 0.02, ' all': 0.02, ' an': 0.02, ' physically': 0.02}
NEW WORD  honest
CURR PRE-NEW CONTEXT I be
NEW CONTEXT I be honest
{' about': 0.02, '.': 0.08, ' here': 0.04, ' –': 0.02, ' with': 0.18, ',"': 0.04, ',': 0.5, ':': 0.04, '...': 0.02, ' that': 0.04, '?': 0.02} {' and': 0.16, ' about': 0.16, '.': 0.06, ' but': 0.02, ' to': 0.06, ' myself': 0.04, '': 0.02, ',': 0.1, ' I': 0.14, ' :': 0.04, ' on': 0.04, ' truthful': 0.04, '...': 0.04, ' with': 0.02, ' in': 0.04, ' as': 0.02}
NEW WORD  on
CURR PRE-NEW CONTEXT I be
NEW CONTEXT I be on
{' you': 0.02, ' her': 0.04, ' top': 0.04, ' your': 0.08, ' board': 0.02, ' his': 0.02, ' an': 0.06, ' it': 0.02, ' the': 0.38, ' my': 0.14, ' a': 0.04, ' this': 0.02, ' vacation': 0.02, ' our': 0.04, ' time': 0.02, ' guard': 0.02, '.': 0.02} {' my': 0.08, ' and': 0.1, ' command': 0.02, ' the': 0.24, ' I': 0.1, ' a': 0.14, ',': 0.08, ' which': 0.02, '.': 0.1, ' board': 0.02, ' an': 0.04, ' for': 0.02, ' myself': 0.02, ' to': 0.02}
NEW WORD  right
CURR PRE-NEW CONTEXT I be
NEW CONTEXT I be right
{' now': 0.02, ' here': 0.24, ' then': 0.02, ' there': 0.16, ' back': 0.16, ',': 0.1, '?': 0.04, ' when': 0.02, ' behind': 0.06, ' for': 0.02, ' down': 0.02, '.': 0.02, ' in': 0.04, ' up': 0.02, ',"': 0.02, ' on': 0.04} {',': 0.14, ' in': 0.06, ' she': 0.02, ' @-@': 0.02, ' the': 0.02, ' I': 0.16, ' right': 0.04, ' (': 0.02, ' "': 0.02, ' her': 0.02, ' when': 0.02, ' on': 0.02, '?': 0.06, ' am': 0.02, ' is': 0.02, ' with': 0.02, '...': 0.02, '.': 0.06, ' my': 0.06, ' but': 0.04, ' you': 0.04, ' that': 0.02, ' for': 0.02, ' and': 0.02, ' myself': 0.02, '!': 0.02}
{' an': 0.05750892392888588, ' "': 0.0, ' honest': 0.18975995549911456, ' on': 0.06548634332889092, ' right': 0.02102085581661816}
JS List [(' "', 0.0), (' right', 0.02102085581661816), (' an', 0.05750892392888588), (' on', 0.06548634332889092), (' honest', 0.18975995549911456)]
highest JS word  honest
CURR CONTEXT I be honest JS 0.14191217859141692
NEW WORD 
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest
{' now': 0.02, ' with': 0.22, ' I': 0.08, ',': 0.38, ' about': 0.02, ';': 0.02, '."': 0.04, ':': 0.08, ' here': 0.02, '?"': 0.02, '.': 0.08, ' -': 0.02} {' about': 0.2, ' and': 0.14, ' to': 0.04, ' I': 0.22, ',': 0.12, ' my': 0.06, '.': 0.04, ' but': 0.02, '!': 0.02, ' of': 0.02, ' About': 0.02, ' myself': 0.02, ' for': 0.02, '...': 0.02, ' a': 0.02, ' with': 0.02}
NEW WORD  about
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest about
{' what': 0.12, ' the': 0.08, ' this': 0.26, ' our': 0.04, ' that': 0.14, ' it': 0.24, ' my': 0.04, ' these': 0.02, ' how': 0.04, ' where': 0.02} {' about': 0.62, ' dealing': 0.02, ' the': 0.18, ' and': 0.04, ' him': 0.02, ' things': 0.04, ' it': 0.02, ' my': 0.02, ' myself': 0.02, ' how': 0.02}
NEW WORD  "
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest "
{'I': 0.24, 'What': 0.02, 'No': 0.04, 'B': 0.02, 'no': 0.04, 'he': 0.04, 'why': 0.02, ' I': 0.02, 'it': 0.06, 'is': 0.04, 'we': 0.04, 'F': 0.02, 'if': 0.02, 'The': 0.08, 'the': 0.04, 'If': 0.02, 'D': 0.02, 'they': 0.02, 'S': 0.02, '\n': 0.02, 'H': 0.02, 'that': 0.04, 'Hey': 0.02, ' "': 0.02, 's': 0.02, 'You': 0.02, 'you': 0.02} {' I': 0.24, ' of': 0.06, ' a': 0.08, ' A': 0.02, ' in': 0.12, " '": 0.04, '.': 0.1, ' with': 0.1, ' that': 0.04, ' on': 0.02, ',': 0.02, ' my': 0.02, '...': 0.04, ' the': 0.04, ' about': 0.02, ' "': 0.02, ' to': 0.02}
NEW WORD  honest
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest honest
{',': 0.38, ' with': 0.24, ' there': 0.02, ',"': 0.06, ' you': 0.02, '.': 0.04, ' if': 0.02, ' about': 0.08, ' and': 0.02, ';': 0.02, ':': 0.04, ' to': 0.02, '?': 0.02, ' I': 0.02} {'.': 0.12, ' of': 0.08, ' about': 0.22, ',': 0.1, ' I': 0.1, ' ;': 0.02, ' and': 0.1, ' in': 0.04, '...': 0.02, ' with': 0.08, '': 0.02, ' truthful': 0.06, ' for': 0.02, ' my': 0.02}
NEW WORD  honestly
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest honestly
{',': 0.56, ' the': 0.02, ' there': 0.04, ' when': 0.04, ';': 0.04, ' this': 0.02, ' that': 0.02, '…': 0.02, '.': 0.1, ' I': 0.1, ' and': 0.02, ' we': 0.02} {' honestly': 0.06, ' about': 0.22, ' told': 0.02, ',': 0.04, ' candid': 0.02, '.': 0.1, ' truthful': 0.16, ' honest': 0.04, ' in': 0.04, ' and': 0.06, ' on': 0.02, ' to': 0.04, ' sincere': 0.02, ' very': 0.02, ' with': 0.04, ' I': 0.02, ' self': 0.04, ' good': 0.02, ' "': 0.02}
{'': 0.20661092083393115, ' about': 0.19568037169143188, ' "': 0.11860013823496605, ' honest': 0.16064113981601902, ' honestly': 0.19514910493086562}
JS List [(' "', 0.11860013823496605), (' honest', 0.16064113981601902), (' honestly', 0.19514910493086562), (' about', 0.19568037169143188), ('', 0.20661092083393115)]
highest JS word 
CURR CONTEXT I be honest JS 0.3200670541259377
NEW WORD  â
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest â
{' a': 0.02, '�': 0.38, 'n': 0.02, '"': 0.1, '€': 0.04, "'": 0.04, 'â': 0.08, ' �': 0.06, ' and': 0.06, '.': 0.02, 'a': 0.02, 'e': 0.02, '?': 0.04, '¢': 0.02, ' ': 0.02, ',"': 0.02, ',': 0.04} {' about': 0.1, ' and': 0.18, ',': 0.04, '...': 0.04, ' self': 0.02, ' with': 0.1, ' to': 0.04, ' on': 0.02, ' I': 0.16, ' a': 0.04, ' what': 0.02, ' honest': 0.02, ' my': 0.02, ' as': 0.02, ' of': 0.02, ' loving': 0.04, '.': 0.02, ' the': 0.04, ' that': 0.02, '': 0.02, ' how': 0.02}
NEW WORD 
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest
{' with': 0.2, ',': 0.6, ' here': 0.04, '?': 0.04, '.': 0.04, ' I': 0.02, ':': 0.02, ' and': 0.02, ' though': 0.02} {' I': 0.16, ' honest': 0.02, ' about': 0.12, ' and': 0.18, ' my': 0.02, ' to': 0.04, ',': 0.1, ' with': 0.06, ' myself': 0.04, ' truthful': 0.02, '.': 0.12, '!': 0.02, '...': 0.02, '': 0.02, ' honestly': 0.02, ' @-@': 0.02, ' as': 0.02}
NEW WORD  about
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest about
{' it': 0.18, ' that': 0.14, ' this': 0.32, ' what': 0.1, ' where': 0.02, ' myself': 0.06, ' the': 0.1, ' who': 0.02, ' something': 0.02, ' my': 0.04} {' things': 0.06, ' the': 0.1, ' it': 0.04, ' about': 0.66, ' her': 0.02, ' a': 0.04, '...': 0.02, ' respecting': 0.02, ' I': 0.02, ' matters': 0.02}
NEW WORD  "
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest "
{'I': 0.2, 'we': 0.06, 'B': 0.02, ' "': 0.04, 'No': 0.06, 'G': 0.02, 'You': 0.06, '\n': 0.08, 'It': 0.04, 'this': 0.02, 'oh': 0.02, 'T': 0.04, 'This': 0.02, 'what': 0.02, 'S': 0.02, 'is': 0.02, 'a': 0.02, 'why': 0.02, 'Hey': 0.04, 'no': 0.04, 'A': 0.04, 'The': 0.02, ' I': 0.02, 'F': 0.02, 'C': 0.02, 'Oh': 0.02} {' to': 0.08, ' of': 0.08, ' I': 0.16, ' "': 0.08, ' very': 0.02, ' for': 0.08, ' you': 0.02, ' in': 0.02, '?': 0.02, '.': 0.06, ' a': 0.04, ' as': 0.02, ' with': 0.04, '...': 0.06, ',': 0.02, '': 0.06, ' about': 0.04, ' on': 0.02, ' that': 0.02, ' my': 0.02, ' The': 0.02, ' A': 0.02}
NEW WORD :
CURR PRE-NEW CONTEXT I be honest
NEW CONTEXT I be honest:
{' that': 0.02, ' I': 0.62, ' It': 0.02, ' That': 0.02, ' This': 0.04, ' The': 0.06, ' when': 0.02, ' it': 0.04, ' there': 0.06, ' You': 0.02, ' We': 0.02, ' the': 0.02, ' If': 0.04} {"I be's": 0.02, 'I be.': 0.06, 'I be in': 0.16, 'I be is': 0.02, 'I be,': 0.2, 'I be I': 0.12, 'I be for': 0.02, 'I be was': 0.02, 'I be and': 0.08, 'I be :': 0.02, 'I be myself': 0.02, 'I be a': 0.04, 'I be...': 0.04, 'I be of': 0.04, 'I be ;': 0.02, 'I be (': 0.02, 'I be but': 0.02, 'I be my': 0.02, 'I be am': 0.02, 'I be to': 0.02, 'I be with': 0.02}
{' â\x80\x93': 0.028990350487376033, '': 0.25025451241246627, ' about': 0.06558091159863703, ' "': 0.05663301226513259, ':': 0.0}
JS List [(':', 0.0), (' â\x80\x93', 0.028990350487376033), (' "', 0.05663301226513259), (' about', 0.06558091159863703), ('', 0.25025451241246627)]
highest JS word 
CURR CONTEXT I be honest JS 0.545350145666798
